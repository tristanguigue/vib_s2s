## Inital Project: Information bottleneck
The paper ([https://arxiv.org/pdf/1612.00410v1.pdf]) is about learning a low dimensional representation of an input x, when there is a supervised learning task to do, i.e. there is a label y. So the low dimensional representation should take into account the target y as well as the input x. The project examines extending this to partially observed time series, i.e. learning a representation of the past that can be used to predict the future.

## March 9th
- Not seq to seq: give whole sequence and predict output
- Change loss function, use LSTM instead

Simple RNN:
$$ h_{t+1} = \tanh(W_{enc}^t h_t + W_{in}^T + b_{enc})$$
$$ \mu_z = W_\mu^T h_{end} + b\mu$$
$$ \log(\Sigma_z) = W_\Sigma^T h_{end} + b\Sigma_z$$

## March 10th
- Goal: Implement stochastic neural net on MNIST with mutual information objectif. Don't regularize with $-\beta I(X, Z)$ but fix low K. Compare with Maximum Likelihood using stochastic gradient variational Bayes. Check if objectif function are equivalent. Use EM?
- Will it forces Z to by Y? No it can't because of the variations in X.

## March 15th
- Compare maximizing mutual information $I(Z, Y)$ with variational inference and maximum likelihood.

## March 18th
- We realized that the mutual information maximzation did not give the same result as maximum likelihood so we need to go back to reading.

## March 30th
Possibilities: 
- MMD: We compare points generated by generative network with true data with gaussian kernel and we update sigma in gaussian kernel as adverserial attack to fool the generative network. Use gaussian kernel instead of moment matching as in paper.
- IB (https://arxiv.org/pdf/1703.00810.pdf): Study neural networks from an information perspectives. Main finding: learning in two phases drift, high gradient and then diffusion (stochastic relaxation).
Apply that to RNN

# March 31st
- Main idea: try to study RNN from information perspective, in particular when there is one output per layer: maximazing I(H1, Y1) as well as between I(H1, Y_FUTURE) while minimizing I(X, H1)?

- First step: FNN with stochastic layer: do we need to explicitly constrain I(X, Z) or is $f_{\theta}(x)$ and number of unit in the hidden layer.

- Understand better information bottleneck: 
    - What does mutual information means in deterministic networks? (https://arxiv.org/pdf/1604.00268.pdf)
    - X -> Y is max MI same as max likelihood?
    - Go back to MI in stochastic FFN: $ <<\log(p(y|z))>_{p(z|x, y)}>_{p(x, y)} $ should be the same as maximum likelihood
    - Why not maximizing directly I(X, Y)?

# May 24th 
- MI is the same as max likelihood for X - > Y
- Sent email to James about expression in stochastic FFN
- In deterministic networks: use sigmoid output as probabilities. DIB objective: use H(Z) instead of I(X, Z)

# May 28th
- Maximising I(X, Z) vs setting dimension of Z & do maximum likelihood: implement for MNIST

# May 30th 
- Try same thing on RNN, guess next pixel.

# June 12th
- Read papers on Normalising Flow https://arxiv.org/abs/1505.05770 and path derivative : https://arxiv.org/pdf/1703.09194v3.pdf try to combine both to get rid of determinant. Much more flexibility

# June 20th
Normalising flow without score function does not work as proposed $f$ is not differentiable. Back to mutual information for RNNs
- Try max I(Z, future output)
- Try overfitting RNN
- Try new parametrisation for r(z)
- Try new graphical model: $z$ <- $x_{1:t}$ -> $y = x_{t+1:T}$ 
- Try linking $z_1$ -> $z_2$ -> $z_3$

# June 27th
- Use $h_{t+2} = \phi(W_h h_{t+1} + W_z z + b)$ might have to do in Theano

# July 11th
- Results for stochastic RNN on binary data:
beta = 0: 0.448, beta = 10^-2: 0.417

# July 12th 
- To overfit try pattern + noise or regression with cubic term
- Use GRU instead of LSTM for stability (at least LSTM instead of RNN)
- Try predict next 15 pixels, do with MNIST (half of image to train)
- Try concatenating the output of the sequence
- More data, longer seq size
- Try again with continuous, for example with cubic pattern from initial point drawn from gaussian and test from initial point far away
- Use smaller bottleneck!

# July 25th
- Try elu + 1 instead of softplus
- Try smaller learning rate
- Try batch norm
- Idea: learning beta? What objective

# August 1st 
- Try with images and compress directly input -> can't do that otherwise Y_i won't be independent from Z_j
- Try different priors for each seq
- Problem, results vary a lot for trial to trial
- Make marginal on z dependent on x?
- Impact of K (with max likelihood, how to get it?) vs impact of beta
- Try MC average over 12 samples

# August 5th
- Checking that sigma decrease for beta = 10^-3, start at 0.68, down to 0.50 then back up to 1. For beta=0, down = 0.02

# August 9th
- Try with seq to seq from images to classes
- Try with generated binary data
- Come up with new data that requires bottleneck
- Verify objective for new pixel prediction

# August 14th
- Seq2Labels and Seq2Label work
- CNN does not give better result




