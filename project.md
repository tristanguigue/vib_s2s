## Inital Project: Information bottleneck
The paper ([https://arxiv.org/pdf/1612.00410v1.pdf]) is about learning a low dimensional representation of an input x, when there is a supervised learning task to do, i.e. there is a label y. So the low dimensional representation should take into account the target y as well as the input x. The project examines extending this to partially observed time series, i.e. learning a representation of the past that can be used to predict the future.

## March 9th
- Not seq to seq: give whole sequence and predict output
- Change loss function, use LSTM instead

Simple RNN:
$$ h_{t+1} = \tanh(W_{enc}^t h_t + W_{in}^T + b_{enc})$$
$$ \mu_z = W_\mu^T h_{end} + b\mu$$
$$ \mu_z = W_\mu^T h_{end} + b\mu$$
$$ \log(\Sigma_z) = W_\Sigma^T h_{end} + b\Sigma_z$$

## March 10th
- Goal: Implement stochastic neural net on MNIST with mutual information objectif. Don't regularize with $-\beta I(X, Z)$ but fix low K. Compare with Maximum Likelihood using stochastic gradient variational Bayes. Check if objectif function are equivalent. Use EM?
- Will it forces Z to by Y? No it can't because of the variations in X.

## March 15th
- Compare maximizing mutual information $I(Z, Y)$ with variational inference and maximum likelihood.

## March 18th
- We realized that the mutual information maximzation did not give the same result as maximum likelihood so we need to go back to reading.

## March 30th
Possibilities: 
- MMD: We compare points generated by generative network with true data with gaussian kernel and we update sigma in gaussian kernel as adverserial attack to fool the generative network. Use gaussian kernel instead of moment matching as in paper.
- IB (https://arxiv.org/pdf/1703.00810.pdf): Study neural networks from an information perspectives. Main finding: learning in two phases drift, high gradient and then diffusion (stochastic relaxation).
Apply that to RNN

# March 31st
- Main idea: try to study RNN from information perspective, in particular when there is one output per layer: maximazing I(H1, Y1) as well as between I(H1, Y_FUTURE) while minimizing I(X, H1)?

- First step: FNN with stochastic layer: do we need to explicitly constrain I(X, Z) or is $f_{\theta}(x)$ and number of unit in the hidden layer.

- Understand better information bottleneck: 
    - What does mutual information means in deterministic networks? (https://arxiv.org/pdf/1604.00268.pdf)
    - X -> Y is max MI same as max likelihood?
    - Go back to MI in stochastic FFN: $ <<\log(p(y|z))>_{p(z|x, y)}>_{p(x, y)} $ should be the same as maximum likelihood
    - Why not maximizing directly I(X, Y)?

# May 24th 
- MI is the same as max likelihood for X - > Y
- Sent email to James about expression in stochastic FFN
- In deterministic networks: use sigmoid output as probabilities. DIB objective: use H(Z) instead of I(X, Z)

# May 28th
- Maximising I(X, Z) vs setting dimension of Z & do maximum likelihood: implement for MNIST

# May 30th 
- Try same thing on RNN, guess next pixel.

# June 12th
- Read papers on Normalising Flow https://arxiv.org/abs/1505.05770 and path derivative : https://arxiv.org/pdf/1703.09194v3.pdf try to combine both to get rid of determinant. Much more flexibility

# June 20th
Normalising flow without score function does not work as proposed $f$ is not differentiable. Back to mutual information for RNNs
- Try max I(Z, future output)
- Try overfitting RNN
- Try new parametrisation for r(z)
- Try new graphical model: $z$ <- $x_{1:t}$ -> $y = x_{t+1:T}$ 
- Try linking $z_1$ -> $z_2$ -> $z_3$

# June 27th
- Use $h_{t+2} = \phi(W_h h_{t+1} + W_z z + b)$ might have to do in Theano



