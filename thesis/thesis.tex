\documentclass[10pt,oneside,openright]{report} 

\title{Sequence to Sequence learning using Variational Information Bottleneck}
\author{Tristan Guigue}
\date{2017}

\usepackage[mastersc]{edmaths}
\usepackage[parfill]{parskip}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{fit, positioning, arrows.meta}
\usepackage[makeroom]{cancel}
\tikzset{
neuron/.style={shape=circle, minimum size=1.1cm,  inner sep=0, draw, font=\small}, io/.style={neuron, fill=gray!20}, deterministic/.style={diamond, minimum size=1.3cm, draw, text badly centered, inner sep=3pt}}
\newtheorem{notation}{Notation}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\begin{document}

\maketitle

\declaration

\begin{abstract}
TODO
\end{abstract}

\tableofcontents

%------------------------------------------------------------------------------------------------
\chapter{Introduction}

TODO
%------------------------------------------------------------------------------------------------
\chapter{Background and Literature Review}
\section{Latent Variables and Generative Models}

A generative model provides assumptions in how the observed data was generated. This generation process is assume to be stochastic in nature, the observed data depending stochastically on other variables. We sometimes introduce hidden variables or latents to better model the generation of the data. Those latent variables can themselves assumed to be generated by observed variable and be used to predict other observed variables as we will see in the information bottleneck method. 

Getting the marginal probability of those latent variables is typically a difficult task if the stochastic mapping with observed variables is complex. To overcome such issues we will use approximation of those distributions and optimise a lower bound to our objective with the hope that our lower bound is close enough to the true objective.

\section{The Information Bottleneck}
\subsection{Regularisation in Supervised Learning}
In supervised learning, we try to infer an output: the random variable $Y$ from an input given by the random variable $X$, learning from labelled training data. The objective is to learn a mapping from $X$ to $Y$ that generalises well to unseen data. To avoid overfitting, that is to ensure the learning process learns the underlying relationship between $X$ and $Y$ rather than noise in the training data, there are different ways to regularise, promoting simpler mappings. We assume here there is indeed an underlying relationship between $X$ and $Y$ so $Y$ must not be independent from the input $X$.

One form of regularisation was introduced by Tishby et al. \cite{tishby}. For each input they seek a stochastic mapping to a representation of the input that provides the most relevant information about the output. They used information theory principles to formalise what a good representation of the input should be extracted. 

The objective is to learn a representation $Z$ that is maximally compressive on $X$ while being maximally informative about the target $Y$. We want to squeeze the information that $X$ contains about $Y$ trough a bottleneck and keep only the most meaningful information about the output. For example when learning the transcript of words from acoustic data, it is possible to greatly compress the input data while still keeping what is useful to predict the words.

\subsection{Mutual Information}

\begin{definition}
The amount of information that $Z$ contains about $Y$ is given by:
$$ I(Z, Y) = \int p(y, z) \log \frac{p(z, y)}{p(z)p(y)} dy\,dz $$ where Y and Z are continous random variables or 
 $$ I(Z, Y) = \sum_y \sum_z p(y, z) \log \frac{p(z, y)}{p(z)p(y)} $$ where Y and Z are discrete random variables 
\end{definition}

\begin{definition}
The KL divergence from a continuous probability distribution $p$ to another continuous probability distribution $q$ is defined as 
$$ KL[p(x)|q(x)] = \int p(x) \log \frac{p(x)}{q(x)} dx $$
\end{definition}

\begin{corollary}
The mutual information of two random variable is equal to the Kullback-Leibler divergence from the joint distribution of the random variables to the product of the distributions.
$$ I(Z, Y) = KL[p(z, y) | p(z)p(y)] $$
\end{corollary}
The mutual information therefore measures how much the joint distribution differs from the joint independent distributions. 

\begin{definition}
The entropy of a distribution $p$ of a continuous random variable $Y$ is given by:
$$ H(Y)  = -\int p(y) \log p(y) dy $$
\end{definition}

\begin{definition}
The conditional entropy of a conditional distribution $p(y|x)$ of a continuous random variable $Y$ given another continuous random variable $Z$ is given by:
$$ H(Y|Z)  = -\int p(y, z) \log p(y|z) dy\,dz $$
\end{definition}
This can be understood as the how much $Y$ varies when $Z$ is fixed

\begin{corollary}
Let $Y$ and $Z$ be random variables:
\begin{equation}
I(Z, Y) = H(Y) - H(Y|Z)
\label{eq:mutual_info_cond_entropy}
\end{equation}
\end{corollary}

\begin{proof}
\begin{align}
I(Z, Y) &= \int p(y, z) \log \frac{p(z, y)}{p(z)p(y)} dy\,dz\\
& = \int p(y, z) \log \frac{p(y|z)}{p(y)} dy\,dz \label{eq:mi_zy}\\
&= \int p(y, z) \log p(y|z) dy\,dz - \int p(y, z) \log p(y) dy\,dz\\ 
&= \int p(y, z) \log p(y|z) dy\,dz - \int p(y) \log p(y) dy\\ 
&= - H(Y|Z) + H(Y)
\end{align}
\end{proof}

When $Y$ and $Z$ are independent the conditional entropy will be the entropy of $Y$ and $I(Z, Y) = 0$. If $Y$ is a deterministic function of $Z$ then $H(Y|Z) = 0$ and $I(Z, Y) = H(Y)$. 

\subsection{Graphical Model}
\begin{notation}
Stochastic nodes are represented as circles and deterministic nodes are represented as diamond. Node of observed data are filled in grey.
\end{notation}

\begin{figure}[H]
\centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
 \node [io] (x) {$X$};
\node [neuron] at (-1, -0.8) (z) {$Z$};
\node [io, below=of x] (y) {$Y$};
\draw [->] (x) -- (z);
\draw [->] (x) -- (y);
\end{tikzpicture}
\caption{Graphical model of the information bottleneck}
\end{figure}

In the information bottleneck model, both the output and the representation depends solely on the input, that is we assume $p(z | x, y) = p(z|x)$. Most importantly, $Z$ is not part of the generative process of $Y$ as it would if $Z$ would be considered a stochastic layer between $Y$ and $Z$. In this case $Z$ is only there to ensure we learn a useful representation of $X$ that will help us to predict $Y$.

The factorisation theorem lets us express the joint distribution as:
$$ p(x, y, z) = p(x)\, p(y|x)\, p(z|x)$$

\subsection{Objective function}
In this section we will assume that the random variables are continuous. We define $p_\theta(z|x)$ as our stochastic encoding of the input where $\theta$ is the encoding parameter. The mutual information of the representation $Z$ and the target $Y$ can then be expressed as:

$$ I(Z, Y|\theta) = \int p(y, z|\theta) \log \frac{p_\theta(z, y)}{p_\theta(z)p(y)} dy\,dz $$

The objective of the information bottleneck method is to maximise $I(Z, Y|\theta)$ while constraining how much information is shared between $X$ and $Z$ such as to force $Z$ to "forget" $X$ and act as a minimal sufficient statistic of $X$ for predicting $Y$. The quantity we are constraining is therefore the mutual information of the input $X$ and its representation $Z$:

$$ I(X, Z|\theta) = \int p_\theta(x, z) \log \frac{p_\theta(x, z)}{p(x)p_\theta(z)} dy\,dz $$

So we want that $$I(X, Z) < I_c$$ where $I_c$ is the information constraint. This can be expressed using a Lagrange multiplier such as to write the whole objective function as:

$$ J_{IB}(\theta) = I(Z, Y|\theta) - \beta I(X, Z|\theta)$$


\subsection{Iterative Solution}
\begin{theorem}
The objective function has an exact formal solution when X, Y and Z are all discrete:
$$ p(z|x) = \frac{p(z)}{Z(x, \beta)} \exp\Big[-\beta \sum_y p(y|x) \log\frac{p(y|x)}{y|z}\Big] $$
\end{theorem}

The proof can be found in \cite{tishby}. The expression $p(y|z)$ can be expressed as 
 
 \begin{align}
p(y|z) &= \sum_x p(y, x|z)  \\
	 & = \sum_x \frac{p(x ,y, z)}{p(z)} \\
	 &= \frac{1}{p(z)} \sum_x p(x) p(y|x) p (z|x) 
\label{eq:y_given_z}
\end{align}

And $p(z)$ can be expressed as a mixture:
 
  \begin{align}
p(z) &= \sum_x p(z, x)  \\
	 &= \sum_x p(x) p(z|x)
\label{eq:z_post}
\end{align}

In the formal solution, the encoder is present on both sides, Tishby et al. present a scheme to find a optimum by iteratively updating $p(z|x)$, $p(y|z)$ and $p(z)$.

\section{Monte Carlo Sampling}
\begin{theorem}[Law of large numbers]
If $X_i$ is a collection of independent identically distributed random variables with density $p(x)$, then
$$ \lim_{N \to \infty} \frac{1}{N} \sum_i f(X_i) = \int f(x) p(x) dx $$
\end{theorem}

In Monte Carlo sampling we use the law of large number to introduce an estimate  of the expectation:
Let $\theta = \mathbb{E}_{x \sim p(x)}[f(X)]$

Then by the law of large number 

$$ \hat{\theta} =  \frac{1}{N} \sum_i f(X_i) $$ is an unbiased estimate of $\theta$.

\section{Recurrent Neural Networks}

\subsection{Architecture}
A neural network is a composition of linear and non-linear transformations organised in layers. It is a deterministic function of the input. The output of the neural network is given by:

$$ \hat{y} = f(x | W) = \sigma_L (W_L h_{L-1}(x)) $$

Where:
\begin{itemize}
\item $\hat{y}$ is the predicted output
\item $\sigma$ is a non-linear function
\item $W$ is the set of weights of the linear transformations
\item $L$ is the index of the output layer
\item $h_l$ are the hidden layers defined as:
  \begin{equation}
    \begin{cases}
          h_1(x) = \sigma_1(W_1 x)\\
          h_l(x) = \sigma_l (W_l h_{l-1}(x)) \quad \forall l \in \{2, L\}\\
    \end{cases}
  \end{equation}
\end{itemize}

\begin{figure}[H]
\centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
 \node [io] (x) {$X$};
\node [deterministic, right=of x] (h1) {$h1$};
\node [deterministic, right=of h1] (h2) {$h2$};
\node [right=of h2](dots) {\ldots};
\node [deterministic, right=of dots] (hL) {$h_L$};
 \node [io, right=of hL] (y) {$Y$};
\draw [->] (x) -- (h1);
\draw [->] (h1) -- (h2);
\draw [->] (h2) -- (dots);
\draw [->] (dots) -- (hL);
\draw [->] (hL) -- (y);
\end{tikzpicture}
\caption{Graphical model of a neural network}
\end{figure}

In a recurrent neural network as introduced by Graves et al \cite{graves}, an input is given at each time step so the data is processed sequentially, this is especially useful to model language, music, stock prices, etc.

In that case the hidden states at each layer are a linear combination of the input and the previous hidden state such as to carry information from the past:
  \begin{equation}
    \begin{cases}
          h_1(x) = \sigma_1(W x) \\
          h_l(x_{1:l}) = \sigma_l (U h_{l-1}(x_{1:l-1}) + W x_l) \quad \forall l \in \{2, L\}\\
    \end{cases}
  \end{equation}

We use the same set of weights: U, W at each step since we are performing the same task with different inputs, this makes the parameter space much smaller.

When dealing with sequential data, there are several possible supervised learning problems:
\subsubsection{Many to many}
\begin{figure}[H]
\centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
\foreach \jlabel [count=\j, evaluate={\jj=int(\j-1); \jd=int(2 * \j);}]  in {l-1,  l, l+1}{
      \node [deterministic] at (\jd, 1) (h-\j) {$h_{\jlabel}$};
      \node [io, above=of h-\j] (x-\j) {$X_{\jlabel}$};
      \node [io, below=of h-\j] (y-\j) {$Y_{\jlabel}$};
      \draw [->] (x-\j) -- (h-\j);
      \draw [->] (h-\j) -- (y-\j);
      \ifnum\j>1
          \draw [->] (h-\jj.east) -- (h-\j.west);
      \fi
} 
\node [left=of h-1] (ldots){\ldots};
\node [right=of h-3] (rdots){\ldots};
\draw [->] (h-3) -- (rdots);
 \draw [->] (ldots) -- (h-1);
\end{tikzpicture}
\caption{Many to many recurrent neural network graphical model}
\end{figure}
In the many to many problem we try to predict the output at each time step based on previous inputs. For example, we try to predict the next pixel in an image, in that case the targets are the next inputs.

$$ \hat{y_l} = f(x_{1:l}|W, U, V) = V h_l(x_{1:l}|W,U)\quad \forall l \in \{1, L\}$$

\subsubsection{Many to one}
\begin{figure}[H]
\centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
\foreach \jlabel [count=\j, evaluate={\jj=int(\j-1); \jd=int(2 * \j);}]  in {1,  2}{
      \node [deterministic] at (\jd, 1) (h-\j) {$h_{\jlabel}$};
      \node [io, above=of h-\j] (x-\j) {$X_{\jlabel}$};
      \draw [->] (x-\j) -- (h-\j);
      \ifnum\j>1
          \draw [->] (h-\jj.east) -- (h-\j.west);
      \fi
} 
\node [right=of h-2] (rdots){\ldots};
\draw [->] (h-2) -- (rdots);
\node [io, above=of h-3] (x-3) {$X_L$};
\node [deterministic, right=of rdots] (h-L) {$h_L$};
\draw [->] (rdots) -- (h-3);
\draw [->] (x-3) -- (h-3);
\node [io, below=of h-3] (y) {$Y$};
\draw [->] (h-L) -- (y);
\end{tikzpicture}
\caption{Many to one recurrent neural network graphical model }
\end{figure}

In the many to one problem, we try to predict the output after having seen a sequence. For example we try to predict the label of an image after sequentially processing all the pixels in the image.

$$ \hat{y} = f(x_{1:L}|W, U, V) = V h_L(x_{1:L}|W,U)$$

\subsubsection{Sequence to sequence}
\begin{figure}[H]
\centering
\begin{tikzpicture}[x=1.1cm, y=1.2cm]
\foreach \jlabel [count=\j, evaluate={\jj=int(\j-1); \jd=int(2 * \j);}]  in {1,  2}{
      \node [deterministic] at (\jd, 1) (h-\j) {$h_{\jlabel}$};
      \node [io, above=of h-\j] (x-\j) {$X_{\jlabel}$};
      \draw [->] (x-\j) -- (h-\j);
      \ifnum\j>1
          \draw [->] (h-\jj.east) -- (h-\j.west);
      \fi
} 
\node [right=of h-2] (rdots){\ldots};
\draw [->] (h-2) -- (rdots);
\node [deterministic, right=of rdots] (h-L) {$h_L$};
\node [io, above=of h-L] (x-L) {$X_L$};
\draw [->] (rdots) -- (h-L);
\draw [->] (x-L) -- (h-L);

\foreach \jlabel [count=\j, evaluate={\jj=int(\j-1); \jd=int(2 * \j);}]  in {1, 2}{
      \node [deterministic] at (8 + \jd, 1) (hpred-\j) {$i_{\jlabel}$};
      \node [io, below=of hpred-\j] (ypred-\j) {$Y_{\jlabel}$};
      \draw [->] (hpred-\j) -- (ypred-\j);
      \ifnum\j>1
          \draw [->] (hpred-\jj.east) -- (hpred-\j.west);
      \fi
} 
\draw [->] (h-L) -- (hpred-1);
\node [right=of hpred-2] (rdots){\ldots};
\draw [->] (hpred-2) -- (rdots);
\node [deterministic, right=of rdots] (hpred-M) {$i_M$};
\node [io, below=of hpred-M] (ypred-M) {$Y_M$};
\draw [->] (rdots) -- (hpred-M);
\draw [->] (hpred-M) -- (ypred-M);
\end{tikzpicture}
\caption{Sequence to sequence graphical model}
\end{figure}

Finally in the sequence to sequence problem we try to predict a sequence based on a partial sequence. For example we try to predict the 10 next pixels after having processed 100 pixels.

\begin{equation}
          \hat{y}_{m} = V i_{m} \quad \forall m \in \{1, M\}\\
\end{equation}

\subsection{Learning}
To learn in a neural network within a supervised learning setting, we introduce a loss function that we intend to minimise.

$$L = \sum_i L(y_i, \hat{y_i} = f(x_i|W))$$

Where $\{(x_i, y_i)\}$ is our training data. 

Since the gradient points in the direction of the steepest descent, the simplest way to learn is by taking the gradient of the loss with respect to each parameter and update their value at each iteration:

$$ W_{l, k+1} = W_{l, k} - \eta \nabla_{W_l} L(y, x, W) \quad \forall l \in \{1, L\}$$

Where $\eta$ is the learning rate.
 
To get the gradient with respect to each parameter $W_l$ we use the chain rule to backpropagate the gradient through the computational graph.
 
 \begin{align}
   \nabla_{W_l} L(y, x, W) &= \frac{\partial L}{\partial h_{L-1}} \nabla_{W_l} h_{L-1}\\
   &= \frac{\partial L}{\partial h_{L-1}} \frac{\partial h_{L-1}}{\partial h_{L-2}} \nabla_{W_l} h_{L-2}\\
   &...\\
   &= \frac{\partial L}{\partial h_{L-1}} \frac{\partial h_{L-1}}{\partial h_{L-2}} ... \frac{\partial h_l}{\partial W_l}
 \end{align}
 
In practice we use more complex optimisation method that take advantage of second-order derivatives. 
 
\subsection{Vanishing Gradients}
In the recurrent neural network setting, our network can be very deep as every input adds a layer. As a consequence gradients will either vanish or explode depending of the initialisation of the weights, this will prevent learning the optimal weights.

\subsection{Gated Recurrent Unit}
To overcome the vanishing or exploding gradient problem, different architecture were proposed including the Gated Recurrent Unit \cite{gru}. Instead of taking $h_l = \sigma(U h_l + W x_l)$ they introduce a gate mechanism that allows a better control of the information that flows through the graph.

The operation performed are the following:
\begin{align}
z_t &= \sigma_g(W_{z} x_t + U_{z} h_{t-1} + b_z) \\
r_t &= \sigma_g(W_{r} x_t + U_{r} h_{t-1} + b_r) \\
h_t &=  z_t \circ h_{t-1} + (1-z_t) \circ \sigma_h(W_{h} x_t + U_{h} (r_t \circ h_{t-1}) + b_h)
\end{align}
Where:
\begin{itemize}
 \item $z_t$ is the update gate that defines how much of the previous memory we keep
 \item $r_t$ is the reset gate that determines how to combine the new input with the previous memory state
\item  $\sigma_g$ is the sigmoid function
\item $\sigma_h$ is a hyperbolic tangent
\end{itemize}

%------------------------------------------------------------------------------------------------
\chapter{Comparison of Supervised Learning Model}

In this chapter we study the relationship between mutual information maximisation and maximum likelihood in different models.

\section{No latent model}

We consider the graphical model of a supervised learning problem where the input is given by the random variable $X$ and the output by the random variable $Y$:

\begin{figure}[H]
\centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
 \node [io] (x) {$X$};
\node [io, below=of x] (y) {$Y$};
\draw [->] (x) -- (y);
\end{tikzpicture}
\end{figure}

We define $p(y|x, \theta)$ as the distribution of the output given the input and our model parameters $\theta$.

The mutual information of $X$ and $Y$ is given by \ref{eq:mutual_info_cond_entropy}:

\begin{align}
I(X, Y) &= H(Y) - H(Y|X)
\end{align}

The entropy of the labels $H(Y)$ is indepedent of our model parameters $\theta$ and will therefore be ignore when taking the gradient with respect to the parameters:

$$ \nabla_\theta I(X,Y) = -\nabla_\theta H(Y|X)$$ 

Using the empirical joint distribution 

$$p(x, y) \approx \frac{1}{N}\sum_n\delta_{x_n}(x)\delta_{y_n}(y)$$, we can approximate the conditional entropy:

\begin{align}
- H(Y|X) &= \int p(x, y) \log p_\theta(y|x) dx\, dy\\
	      &\approx \int \frac{1}{N}\sum_n\delta_{x_n}(x)\delta_{y_n}(y) \log p_\theta(y|x)dx\, dy\\
	      &= \frac{1}{N} \sum_n \log p_\theta(y_n|x_n,)
\end{align}

The log likelihood is given by:
\begin{align}
 l[p_\theta(y|x)] &= \log \prod_n p_\theta(y_n|x_n)\\
 	      &= \sum_n \log p_\theta(y_n|x_n)
\end{align}

We can conclude that mutual information maximisation under an empirical distribution is equivalent to maximum likelihood in the supervised setting:

\begin{align}
\nabla_\theta I(X,Y) = 0 \Leftrightarrow \nabla_\theta l[p_\theta(y|x)] = 0
\end{align}

\section{Stochastic Feed-forward Network}

We now consider a stochastic feed-forward network model that contains an encoder $p_\theta(z|x)$ and a decoder $p_\theta(y|z)$. This is different from the information bottleneck setting as in this case $Z$ is a latent variable part of the generative process of the outputs rather than a representation of the input. This can be useful to learn how to synthesize likely output with a given input similarely to a Variational Auto-Encoder \cite{vae} but in a supervised setting.

\begin{figure}[H]
\centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
 \node [io] (x) {$X$};
\node [neuron, below=of x] (z) {$Z$};
\node [io, below=of z] (y) {$Y$};
\draw [->] (x) -- (z);
\draw [->] (z) -- (y);
\end{tikzpicture}
\end{figure}

\subsection{Maximum Likelihood}

We can simply use a Monte Carlo approximation and sample from the encoder and pass the samples through the decoder:

\begin{align}
 p(y|x) &= \int p_\theta(y, z|x) dz \\
 &= \int p_\theta(y|z) p_\theta(z|x) dz \\
 &= E_{Z | X}[p_\theta(y|z)] 
\end{align}

We can approximate the expectation using Monte Carlo:
\begin{align}
 p_\theta(y|x) &\approx \frac{1}{M} \sum^M_{m=1} p(y | z^{(m)}) 
\end{align}

Where $ z^{(m)} \sim p_\theta(z|x) $

However this estimator have a very high variance in high dimensional space because our draw from $p_\theta(z | x)$ might not contribute significantly to our estimate $p(y |x)$. For most $z$, $p_\theta(y|z)$ will be nearly zero. We would need an extremely large number of samples to ensure we will generate samples that contribute to $p(y|x)$, this makes it impractical.

\subsubsection{Stochastic Gradient Variational Bayes}

To solve this, we would like to sample values of $Z$ that are likely to have produced $Y$. To do so we introduce a variational approximation $q_\phi(z|x, y)$ of $p(z|x, y)$ and optimise the variational lower bound with respect to the variational parameters $\phi$.
 
\begin{align}
 \log p(y|x) &= \log(\int p_\theta(y, z|x) dz) \\
 & =  \log(\int q_\phi(z|x, y) \frac{p_\theta(y, z|x)}{q_\phi(z|x, y)}) \\
 & \geq \int q_\phi(z|x, y) \log \frac{p_\theta(y, z|x)}{q_\phi(z|x, y)}
\end{align}

Using Jensen's inequality.

\begin{align}
 \log p(y|x) & \geq \int q_\phi(z|x, y) \log \frac{p_\theta(z|x) p_\theta(y| z)}{q_\phi(z|x, y)} \\
  &= \int q_\phi(z|x, y) \log \frac{p_\theta(z|x)}{q_\phi(z|x, y)}  + \int q_\phi(z|x, y) \log p_\theta(y|z) \\
  &= E_q[p_\theta(y|z)] - KL\big[q_\phi(z|x, y) || p_\theta(z|x)]\\
  &\approx  \frac{1}{M} \sum^M_{m=1} p(y | z^{(m)}) - KL\big[q_\phi(z|x, y) || p_\theta(z|x)\big]
\end{align}
Where $ z^{(m)} \sim q_\phi(z|x, y)$, using a Monte Carlo approximation of the expectation and assuming the KL divergence can be computed analytically.

So we try to maximise the expected value of decoder given z sampled from a distribution parametrised by the encoder while minimising the KL divergence between $q_\phi(z|x, y)$ and $p_\theta(z|x)$. which act as a regulariser. The bound is exact when $q_\phi(z| x, y) = p(z|x, y)$

\subsection{Maximum Mutual Information}

We would like to compare this result to the maximisation of the  mutual information between Z and Y:

\begin{align}
I(Z, Y|\theta) &= \int p_\theta(y, z) \log \frac{p_\theta(z, y)}{p_\theta(z)p(y)} dy\,dz\\
& = \int p_\theta(y, z) \log \frac{p_\theta(y|z)}{p(y)} dy\,dz\\
&= \int p_\theta(y, z) \log p_\theta(y|z) dy\,dz - H(y) 
\end{align}
We ignore the entropy of the labels as it is independent of our model parameters. We get:
\begin{align}
I(Z, Y|\theta) &= \int p_\theta(x, y, z) \log p_\theta(y| z) dx\,dy\,dz + C\\
&= \int p_\theta(z | x, y) p(x, y) \log p_\theta(y | z) dx\,dy\,dz + C
\end{align}
We can approximate use the empirical joint distribution of the data and the labels:
$$ p(x, y) \approx \frac{1}{N}\sum_n \delta_{x_n}(x) \delta_{y_n}(y)$$
So we get:
$$ I(Z, Y) \approx \frac{1}{N} \int p(z | x^{(n)}, y^{(n}) \log p_\theta(y^{(n)} | z) + C$$

As we can see this is not equivalent to the maximum likelihood results. In particular we have no way to easily get the posterior over z. 

%------------------------------------------------------------------------------------------------
\chapter{Model}
\section{Deep Variational Information Bottleneck}

In the previous section a solution to the information bottleneck problem was presented when X, Y and Z are all discrete. However in the general case there won't be a formal solution or an iterative method to find the parametric encoder. Alemi at al. in \cite{vib} introduce a variational approach to the information bottleneck. They use variational inference to provide a lower bound to the information bottleneck objective. This allow us to use stochastic gradient ascent to find an optimum of the lower bound and to parametrise the encoder with a neural network which can handle a broad range of data.

\subsection{Variational Lower Bound}

The equation \ref{eq:y_given_z} is not tractable in the continuous case. Indeed
\begin{align}
p_\theta(y|z) &= \frac{1}{p(z)} \int_x p(x, y) p_\theta(z|x) 
\end{align}
And $p_\theta(z|x)$ will typically be a very complex distribution, for example using a neural network to get the parameter of the distribution.

Using $q(y|z)$ as a variational approximation to $p(y|z)$ we can find a lower bound to the mutual information of $Y$ and $Z$ as proposed in Barber et al. \cite{barber}.

\begin{lemma}
A lower bound to the mutual information of $Y$ and $Z$ is:
$$ \mathcal{L}^{A}(x, y, \theta, \phi) = \int p(x)\, p(y|x)\, p_\theta(z|x) \log q_\phi(y|z) dy\, dz\, dx + H(Y)$$
\end{lemma}

\begin{proof}
Given that the KL divergence is necessarily positive we have 

$$ KL\big[p(y|z)|q_\phi(y|z)\big] \geq 0$$
Therefore 
$$ \int p(y|z) \log \frac{p(y|z)}{q_\phi(y|z)} dy \geq 0 $$
$$ \Rightarrow \int p(y|z) \log p(y|z) dy \geq \int p(y|z) \log q_\phi(y|z) dy$$
$$ \Rightarrow \frac{1}{p(z)} \int p(y, z) \log p(y|z) dy \geq  \frac{1}{p(z)} \int p(y, z) \log q_\phi(y|z) dy$$
$$\Rightarrow  \int p(y, z) \log p(y|z) dy \geq  \int p(y, z) \log q_\phi(y|z) dy$$
$$\Rightarrow  \int p(y, z) \log p(y|z) dy\, dz\geq  \int p(y, z) \log q_\phi(y|z) dy\, dz$$
$$\Rightarrow  \int p(y, z) \log p(y|z) dy\, dz - \int p(y, z) \log p(y) dy\, dz\geq  \int p(y, z) \log q_\phi(y|z) dy\, dz - \int p(y, z) \log p(y) dy\, dz$$

 \begin{align}
 \Rightarrow I(Z, Y) &\geq  \int p(y, z) \log q_\phi(y|z) dy\, dz + H(Y)\\
	  &=  \int p(x, y, z) \log q_\phi(y|z) dy\, dz\, dx + H(Y)\\
	  &=  \int p(x) p(y|x) p_\theta(z|x) \log q_\phi(y|z) dy\, dz\, dx + H(Y)
\end{align}
\end{proof}
Note that since the entropy of the label is a constant, it will be ignored in our objective function.

If $q_\phi(z|y) = p(z|y)$, the bound is exact. We assume that $KL\big[q_\phi(z|y)|p(z|y)\big]$ tend to zero that is our approximation converges to the true posterior.

We also approximate the marginal on $Z$ $p(z)$ given in equation \ref{eq:z_post} by $r_\psi(z)$ which gives us the second part of the objective function.

\begin{lemma}
An upper bound to the mutual information of $X$ and $Z$ is:
$$ \mathcal{U}(x, y, \theta, \psi) = \int p(x)p(y|x)p_\theta(z|x) \log \frac{p_\theta(z|x)}{r_\psi(z)}dx\, dy\, dz$$
\end{lemma}

\begin{proof}
$$ KL[p(z)|r(z)] \geq 0$$
Therefore 
$$ \int p(z) \log \frac{p(z)}{r(z)} dz \geq 0 $$
$$ \Rightarrow \int p(z) \log p(z) dz\geq \int r(z) \log p(z) dz$$
$$ \Rightarrow \int p(z, x) \log p(z) dx\, dz \geq \int r(z, x) \log p(z) dx\, dz$$
$$ \Rightarrow -\int p(z, x) \log p(z) dx\, dz\leq - \int r(z, x) \log p(z) dx\, dz$$
$$ \Rightarrow \int p(z, x) \log p(z|x)dx\, dz -\int p(z, x) \log p(z) dx\, dz\leq  \int p(z, x) \log p(z|x)dx\, dz - \int r(z, x) \log p(z) dx\, dz $$
 \begin{align}
I(X, Z) & \leq  \int p(z, x) \log \frac{p(z|x)}{r(z)} dx\, dz\\
   	  & =  \int p(x, y, z) \log \frac{p(z|x)}{r(z)}dx\, dy\, dz \\
   	  & = \int p(x)p(y|x)p(z|x) \log \frac{p(z|x)}{r(z)}dx\, dy\, dz \\
\end{align}
\end{proof}

\begin{theorem}
A lower bound to the information bottleneck objective function can be written as:
 \begin{align}
\mathcal{L}(x, y, \theta, \phi, \psi) = \int p(x) p(y|x) p_\theta(z|x) \Big[ \log q_\phi(y|z) - \beta  \log \frac{p_\theta(z|x)}{r_\psi(z)}\Big] dx\, dy\, dz
\end{align}
\end{theorem}

One possibility to to find an optimum to this problem would be to update iteratively the model parameter $\theta$ from $p_\theta(z|x)$  and the variational parameters $\phi$ and $\psi$ from $q_\phi(y|z)$ and $r_\psi(z)$ in a procedure similar to the expectation maximisation algorithm for the maximum likelihood. However in this case we will use stochastic gradient descent to optimise the bound.

\subsection{Estimating the lower bound}
Since X and Y are known during training, we can use the empirical data distribution $\tilde{p}(x, y) = \frac{1}{N}\sum_n\delta_{x_n}(x)\delta_{y_n}(y)$ to approximate the lower bound:

 \begin{align}
\tilde{\mathcal{L}} & = \int \tilde{p}(x, y) p_\theta(z|x) \Big[\log q_\phi(y|z) - \beta \log \frac{p_\theta(z|x)}{r_\psi(z)} \Big]dx\, dy\, dz  \\
   & = \int \frac{1}{N}\sum_n\delta_{x_n}(x)\delta_{y_n}(y) p_\theta(z|x) \Big[\log q_\phi(y|z) - \beta \log \frac{p_\theta(z|x)}{r_\psi(z)} \Big] dx\, dy\, dz \\
   & = \frac{1}{N}\sum_n \int \delta_{x_n}(x)\delta_{y_n}(y) p_\theta(z|x) \Big[ \log q_\phi(y|z) - \beta \log \frac{p_\theta(z|x)}{r_\psi(z)}\Big ] dx\, dy\, dz \\
   & = \frac{1}{N}\sum_n \int p_\theta(z|x_n) \Big[  \log q_\phi(y_n|z_n) - \beta   \log \frac{p_\theta(z|x_n)}{r_\psi(z)} \Big] dz \label{eq:lower}
\end{align}


\subsection{Reparametrisation Trick}
We now need to find a way to propagate the gradient through equation \ref{eq:lower}. 

One way to do this would be using the score function:
 \begin{align}
\nabla_\theta \mathbb{E}_{p_\theta(z|x)}\big[ g(z) \big] &= \int_z \nabla_\theta p_\theta(z|x)  g(z)\\
 &= \int_z \frac{p_\theta(z|x)}{p_\theta(z|x)} \nabla_\theta p_\theta(z|x)  g(z) \\
 &= \int_z p_\theta(z|x) \nabla_\theta \log p_\theta(z|x)  g(z)\\
&= \mathbb{E}_{p_\theta(z|x)}\big[ \nabla_\theta \log p_\theta(z|x) g(z)  \big]\\
&\approx \frac{1}{L} \sum_l  g(z^{(l)}) \nabla_\theta \log p_\theta(z^{(l)}|x)
\end{align}

However it has been shown that this gradient estimator exhibits very high variance and is therefore impractical.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
 \node [neuron] (z) {$Z$};
\node [io, below=of z] (x) {$X$};
\node [deterministic, left=of x] (theta) {$\theta$};
\node [deterministic, above=of z] (l) {$L$};
\draw [->] (x) -- (z);
\draw [->] (theta) -- (z);
\draw [->] (z) -- (l);
\end{tikzpicture}
        \caption{Initial Model}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
 \node [deterministic] (z) {$Z$};
\node [io, below=of z] (x) {$X$};
\node [deterministic, left=of x] (theta) {$\theta$};
\node [deterministic, above=of z] (l) {$L$};
\node [neuron, right=of x] (eps) {$\epsilon$};
\draw [->] (x) -- (z);
\draw [->] (theta) -- (z);
\draw [->] (z) -- (l);
\draw [->] (eps) -- (z);
\end{tikzpicture}
        \caption{Reparametrised Model}
    \end{minipage}
\end{figure}

Another way to solve this issue is to use the reparametrisation trick also called pathwise estimator  from Kingma \& Welling \cite{kingma} and write $z$ as a deterministic function of the model parameter $\theta$ and a random variable $\epsilon$: $z = f(\theta(x), \epsilon)$ and
$$p(z) = p(\epsilon) \Big|\frac{d\epsilon}{dz} \Big| \Rightarrow |p(\epsilon) d\epsilon| = |p(z|x) dz|$$
The gradient can then be propagated back to the input and the parameters through the deterministic node $Z$:
\begin{align}
\nabla_\theta \int p_\theta(z|x) g(z) dz &= \nabla_\theta \int p(\epsilon) g\big(f(\theta(x), \epsilon)\big) d\epsilon\\
&= \int p(\epsilon) \nabla_\theta g\big(f(\theta(x), \epsilon)\big) d\epsilon\\
&= \mathbb{E}_{p(\epsilon)}\Big[\nabla_\theta g\big(f(\theta(x), \epsilon)\big)\Big]
\end{align}

We can therefore rewrite the estimator as:

 \begin{align}
\tilde{\mathcal{L}}   &= \frac{1}{N}\sum_n  \mathbb{E}_{\epsilon \sim p(\epsilon)}\Big[  \log q(y_n|f(x_n, \epsilon)) - \beta  \log \frac{p(f(x_n, \epsilon)|x_n)}{r(f(x_n, \epsilon))}\Big]  \label{eq:lower2}
\end{align}

Note that if the the KL divergence can be calculated analytically as it is the case if both distribution are gaussian we can write more simply:

 $$ \tilde{\mathcal{L}}  = \frac{1}{N}  \sum_{n=1}^{N} \mathbb{E}_{\epsilon \sim p(\epsilon)}\Big[\log q(y_{n} |f(x_{n}, \epsilon))\Big] - \beta KL[p(Z|x_{n}), r(Z)]$$

To turn this into a convex problem, the obective function we try to minimise is:

 $$ J_{VIB} = \frac{1}{N}  \sum_{n=1}^{N} \mathbb{E}_{\epsilon \sim p(\epsilon)}\Big[-\log q(y_{n} |f(x_{n}, \epsilon))\Big] + \beta KL\big[p(Z|x_{n}), r(Z)\big]$$


The first part of of the lower bound correspond to the cost of constructing $Y$ under our model from $Z$. The second part is a regularisation term that correspond to the number of bits we need to construct $Z$.

\section{Application to Sequential Data}

We apply the variational information bottleneck method to find a good representation of a sequence rather than a single input. This is useful to avoid overfitting on the training data when predicting future outputs. We will use recurrent neural networks to combine the inputs and pass the hidden layer of the network through an information bottleneck to get a robust representation of the sequence. As we have seen there are different supervised learning problem for sequential data we will see how to apply the variational information bottleneck in each setting.

\subsection{Many to many}
\begin{figure}[H]
\centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
\foreach \jlabel [count=\j, evaluate={\jj=int(\j-1); \jd=int(2 * \j);}]  in {t-1,  t, t+1}{
      \node [deterministic] at (\jd, 1) (h-\j) {$h_{\jlabel}$};
       \node [neuron] at (\jd + 1, 1.5) (z-\j) {$Z_{\jlabel}$};
      \node [io, above=of h-\j] (x-\j) {$X_{\jlabel}$};
      \node [io, below=of h-\j] (y-\j) {$Y_{\jlabel}$};
      \draw [->] (x-\j) -- (h-\j);
      \draw [->] (x-\j) -- (z-\j);
      \draw [->] (h-\j) -- (y-\j);
      \ifnum\j>1
          \draw [->] (h-\jj.east) -- (h-\j.west);
      \fi
} 
\node [left=of h-1](ldots) {\ldots};
\node [right=of h-3](rdots) {\ldots};
\draw [->] (ldots) -- (h-1);
\draw [->] (h-3) -- (rdots);
\end{tikzpicture}
\caption{Graphical model of many to many variational information bottleneck}
\end{figure}

In the many to many problem we try to find at each time step a representation $Z_t$ that is maximally compressive of each input $X_t$ and maximally predictive about future outputs. The objective function can therefore be written as:

$$ J_{IB} = \sum_t I(Z_t, Y_{t}) - \beta I(X_{t}, Z_t)$$
 
 The problem with this setting is that the representation $Z_t$ does not contain information about the past, therefore they won't be good predictor of the outputs. One way to solve this would be to have $Z_t$ depend on all previous inputs. However in that case the $Z_t$ won't be independent and therefore we wouldn't be able to decompose the objective function as we did.
 
\subsection{Many to one}

\begin{figure}[H]
\centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
 \node [io] (x) {$X_{1:T}$};
\node [neuron] at (-1, -0.8) (z) {$Z$};
\node [io, below=of x] (y) {$Y$};
\draw [->] (x) -- (z);
\draw [->] (x) -- (y);
\end{tikzpicture}
\caption{Graphical model of many to one variational information bottleneck}
\end{figure}

The many to one problem is similar to the original supervised problem. Indeed we can consider the whole sequence as a random variable and apply the information bottleneck on this. When implementing this we will use the output of a recurrent neural network and pass it through the information bottleneck to get a good representation of the sequence. 

\begin{figure}[H]
\centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
\foreach \jlabel [count=\j, evaluate={\jj=int(\j-1); \jd=int(2 * \j);}]  in {1,  2}{
      \node [deterministic] at (\jd, 1) (h-\j) {$h_{\jlabel}$};
      \node [io, above=of h-\j] (x-\j) {$X_{\jlabel}$};
      \draw [->] (x-\j) -- (h-\j);
      \ifnum\j>1
          \draw [->] (h-\jj.east) -- (h-\j.west);
      \fi
} 
\node [right=of h-2] (rdots){\ldots};
\draw [->] (h-2) -- (rdots);
\node [io, above=of h-3] (x-3) {$X_T$};
\node [deterministic, right=of rdots] (h-L) {$h_T$};
\draw [->] (rdots) -- (h-3);
\draw [->] (x-3) -- (h-3);
\node [neuron] at (7, 3.5) (z) {$Z$};
\draw [->] (x-1.north) -- (z);
\draw [->] (x-2.north) -- (z);
\draw [->] (x-3.north) -- (z);
\node [io, right=of h-L] (y) {$Y$};
\draw [->] (h-L) -- (y);
\end{tikzpicture}
\caption{Graphical model of many to one variational information bottleneck }
\end{figure}

If we draw the graphical model as a sequence we note that the hidden states represented are the hidden states of the generative model rather than the hidden state of the encoder. 

The information bottleneck objective is then:
$$ J_{IB} = I(Z, Y) - \beta I(X_{1:T}, Z)$$
Whose lower bound objective under the variational approximation becomes:
 $$ J_{VIB} = \frac{1}{N}  \sum_{n=1}^{N} \mathbb{E}_{\epsilon \sim p(\epsilon)}[- \log q(y^{n} |f(x_{1:T}^n, \epsilon))] + \beta KL[p(Z|x_{1:T}^n), r(Z)]$$

\subsection{Sequence to Sequence}
Sequence to sequence problems have first been used in the context of recurrent neural network by Sutskever \cite{s2s}. They are especially useful for translation. The original model compress the whole input sequence into a hidden states from which the output sequence is generated. This makes it a good candidate for the information bottleneck framework as pulling as relevant representation of the input is crucial to be able to generate the right output sequence. Recent model have included attention \cite{attention} or other external memory mechanism to go around this bottleneck.

\begin{figure}[H]
\centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
 \node [io] (x) {$X_{1:T}$};
\node [neuron] at (-1, -0.8) (z) {$Z$};
\node [io, below=of x] (y) {$Y_{1:S}$};
\draw [->] (x) -- (z);
\draw [->] (x) -- (y);
\end{tikzpicture}
\caption{Sequence to sequence variational information bottleneck graphical model}
\end{figure}

The graphical model of this problem is also similar to the original information bottleneck graphical model if we consider the input and output sequences as random variables. Representing the hidden states of the graphical model we get: 

\begin{figure}[H]
\centering
\begin{tikzpicture}[x=1.1cm, y=1.2cm]
\foreach \jlabel [count=\j, evaluate={\jj=int(\j-1); \jd=int(2 * \j);}]  in {1,  2}{
      \node [deterministic] at (\jd, 1) (h-\j) {$h_{\jlabel}$};
      \node [io, above=of h-\j] (x-\j) {$X_{\jlabel}$};
      \draw [->] (x-\j) -- (h-\j);
      \ifnum\j>1
          \draw [->] (h-\jj.east) -- (h-\j.west);
      \fi
} 
\node [right=of h-2] (rdots){\ldots};
\draw [->] (h-2) -- (rdots);
\node [deterministic, right=of rdots] (h-L) {$h_T$};
\node [io, above=of h-L] (x-L) {$X_T$};
\draw [->] (rdots) -- (h-L);
\draw [->] (x-L) -- (h-L);

\foreach \jlabel [count=\j, evaluate={\jj=int(\j-1); \jd=int(2 * \j);}]  in {1, 2}{
      \node [deterministic] at (8 + \jd, 1) (hpred-\j) {$i_{\jlabel}$};
      \node [io, below=of hpred-\j] (ypred-\j) {$Y_{\jlabel}$};
      \draw [->] (hpred-\j) -- (ypred-\j);
      \ifnum\j>1
          \draw [->] (hpred-\jj.east) -- (hpred-\j.west);
      \fi
} 
\draw [->] (h-L) -- (hpred-1);
\node [right=of hpred-2] (rdots){\ldots};
\draw [->] (hpred-2) -- (rdots);
\node [deterministic, right=of rdots] (hpred-M) {$i_S$};
\node [io, below=of hpred-M] (ypred-M) {$Y_S$};
\draw [->] (rdots) -- (hpred-M);
\draw [->] (hpred-M) -- (ypred-M);
\node [neuron] at (10, 4) (z) {$Z$};
\draw [->] (x-1.north) -- (z);
\draw [->] (x-2.north) -- (z);
\draw [->] (x-L.north) -- (z);
\end{tikzpicture}
\caption{Sequence to sequence variational information bottleneck graphical model}
\end{figure}

The information bottleneck objective is then:
$$ J_{IB} = I(Z, Y_{1:S}) - \beta I(X_{1:T}, Z)$$
Whose lower bound objective under the variational approximation becomes:
 $$ J_{VIB} = \frac{1}{N}  \sum_{n=1}^{N} \mathbb{E}_{\epsilon \sim p(\epsilon)}[- \log q(y_{1:S}^n |f(x_{1:T}^n, \epsilon))] + \beta KL[p(Z|x_{1:T}^n), r(Z)]$$

\chapter{Implementation}
\section{Stochastic encoder $p_\theta(z|x)$}
Like in \cite{vib} we choose our encoder to be a gaussian whose mean and variance will be given by the output of a neural network:
$$ p(z|x) \sim \mathcal{N}(z | f^\mu(x), f^\Sigma(x))$$ where $f^\mu$ and $f^\Sigma$ are multi-layer perceptrons. The last layer of the perceptron is processed by two sets of linear weights, the first gives the mean and we apply a softplus to the second which gives the diagonal standard deviation terms. The covariance is chosen to be diagonal such as to reduce the size of the output and to force the features learned to be uncorrelated.

\paragraph{Reparametrisation trick}
Since the encoder is a gaussian we can use  $z = f(x, \epsilon) =  \mu(x) + \Sigma(x)^{1/2} \epsilon$ for the reparametrisation trick where $\epsilon \sim \mathcal{N}(0, I)$. 
 
\section{Decoder $q_\phi(y|z)$}
For the variational approximation of $p(y|z)$ we use a simple logistic regression:

$$ q_\phi(y|z) \sim \mathcal{C}(y| \mathcal{S}(Wz + b)))$$ where $\mathcal{C}$ is a categorical distribution and $\mathcal{S}$ is the softmax function. Note that we won't actually sample from the categorical distribution we will rather take the expected values for each categories given by the softmax and feed those values to our cross-entropy loss function.

\section{Marginal $r_\psi(z)$}

As an approximation to the marginal we can use as a simple non-parametric approximation:
$$r_\psi(z) \sim \mathcal{N}(0, I)$$

However to improve the expressiveness of the prior distribution on the latent, we can allow its parameters to be learned. 

$$r_\psi(z) \sim \mathcal{N}(\mu_r, \Sigma_r)$$

In the objective function we calculate the KL divergence from $p_\theta(z|x) \sim \mathcal{N}(\mu_p, \Sigma_p)$ to $r_\psi(z) \sim \mathcal{N}(\mu_r, \Sigma_r)$. Since both are assumed to be gaussians with diagonal variance we can use:

\begin{align}
KL[\mathcal{N}_p| \mathcal{N}_r] &= \frac{1}{2}(Tr(\Sigma_r^{-1}\Sigma_p) + (\mu_r - \mu_p)^T\Sigma_r^{-1}(\mu_p - \mu_r) - k + \log\frac{|\Sigma_r|}{|\Sigma_p|})\\
&= \frac{1}{2}(\sum_i \frac{\Sigma_{p, ii}}{\Sigma_{r, ii}} + \sum_i \frac{(\mu_{r_i} - \mu_{p, i})^2}{\Sigma_{r, ii}} - k + \log\frac{\prod \Sigma_{r, ii}}{\prod \Sigma_{p, ii}})\\
&= \frac{1}{2}(\sum_i[ \log \Sigma_{r, ii} - \log \Sigma_{p, ii} - 1 + \frac{\Sigma_{p, ii}}{\Sigma_{r, ii}} + \frac{(\mu_{r_i} - \mu_{p, i})^2}{\Sigma_{r, ii}}]
\end{align}

In the case where $r_\psi(z)$ is a standard normal prior we get:
\begin{align}
KL[\mathcal{N}_p | \mathcal{N}_0] &= \sum_i[ -\log \Sigma_{p, ii} - 1 + \Sigma_{p, ii}+ \mu_{p, i}^2]
\end{align}

\section{Monte Carlo Expectation}

In the lower bound we have to take an expectation with respect $p(\epsilon)$. To do so we take S Monte Carlo samples: 
 $$ \mathbb{E}_{\epsilon \sim p(\epsilon)}[- \log q(y_{n} |f(x_{n}, \epsilon))] \approx \frac{1}{S}\sum_s - \log q(y_{n} |f(x_{n}, \epsilon_s)) $$
 
 In practice, we use S = 1 or S = 12. Taking more samples would give a better estimate however since we are using stochastic gradient descent, we are already averaging on the batches and therefore have a doubly stochastic gradient estimator.

\section{Computational Graph}
\includegraphics[scale=0.4]{model}


%------------------------------------------------------------------------------------------------
\chapter{Results}
\section{One to one}
\subsection{MNIST image to label}

Using the same parameters as in the Deep Variational Information Bottleneck paper, the objective of the experiment was to check that we could reproduce similar results and that the information bottleneck mathod is effective.

\begin {table}[H]
\begin{center}
\begin{tabular}{ c | c  }
 Learning rate & $10^{-3}$ \\
 Hidden units & 1024 \\
 Bottleneck size & 256 \\
\end{tabular}
\end{center}
\caption{Parameters}
\end {table}

\begin {table}[H]
\begin{center}
\begin{tabular}{ c | c c }
 $\beta$ & accuracy & loss\\
  \hline
0 & 98.26 & 0.064\\
$10^{-4}$ & 98.52 & 0.055\\
$5.10^{-4}$ & 98.62 & 0.053 \\
\end{tabular}
\end{center}
\caption{Results}
\end {table}

\subsection{Behavior when $\beta = 0$}
When $\beta = 0$ we expect the network to become fully deterministic as there is no restriction on $p_\theta(z|x)$, we verify in this experiment that the values of the diagonal covariance matrix tend to zero as the network is trained:

\begin {table}[H]
\begin{center}
\begin{tabular}{ c | c c }
 $\beta$ & $\bar{\Sigma_{ii}}$ \\
 \hline
0 & 0.02\\
1 & 0.99 \\
\end{tabular}
\end{center}
\caption{Results}
\end {table}

After training the network for 200 epochs the average diagonal values of the covariance matrix are indeed close to 0 in the non-regularised case whereas with a very strong regularisation they will be very close to the variational parameters: $r(z) \sim \mathcal{N}(0, I)$

\subsection{Bottleneck size considerations}
In this experiment we would like to verify that modifying the bottleneck size does not have the same effect as setting a non-zero regularisation parameter.

\begin {table}[H]
\begin{center}
\begin{tabular}{ c | c c }
 K & accuracy & loss \\
 \hline
256 & 98.26 & 0.064 \\
128 & 98.14 & 0.065 \\
64 & 98.12& 0.067 \\
32 & 98.09 & 0.066 \\
\end{tabular}
\end{center}
\caption{Results}
\end {table}

We see that reducing the bottleneck size does not have the same effect as applying the information bottleneck regularisation. We see no improvement of the accuracy and loss. This is a confirmation that using the mutual information to control the amount of information we let flow from the input to the output is a better quantifier.

\section{Many to one}
\subsection{Sequence of MNIST images to single label}
In this experiment we try to predict the label of a first image of a sequence of pixel so the network has to remember the first image it has seen in associate a label to it.


\begin {table}[H]
\begin{center}
\begin{tabular}{ c | c  }
 Learning rate & $10^{-4}$ \\
 Batch size & 500\\
 Hidden units & 128 \\
 Bottleneck size & 32 \\
 Sequence length & 5 \\
 Monte Carlo samples & 12
\end{tabular}
\end{center}
\caption{Parameters}
\end {table}

\begin {table}[H]
\begin{center}
\begin{tabular}{ c | c c }
 $\beta$ & accuracy & loss \\
 \hline
0 & 93.95 & 0.313 \\
$10^{-3}$ & 95.35 & 0.27 \\
\end{tabular}
\end{center}
\caption{Results}
\end {table}

We see here a strong improvement of the performance of our classifier when using a non-zero regularisation parameter. 

\subsection{Video of shapes to number of frames seen}


\section{Sequence to sequence}
\subsection{Sequence of MNIST images to sequence of labels}
\paragraph{Using Gated Recurrent Unit}
\paragraph{Using Convolutional Neural Network}
\subsection{Translation}

%------------------------------------------------------------------------------------------------
\chapter{Conclusion}
TODO

\appendix
%------------------------------------------------------------------------------------------------
\chapter{First Appendix}

\begin{thebibliography}{9} 
\bibitem{barber}
David Barber and Felix Agakov
\textit{The IM Algorithm : A variational approach to Information Maximization}
NIPS 2004
\bibitem{tishby} 
N. Tishby, F.C. Pereira, and W. Biale.
\textit{The information bottleneck method}.
37th annual Allerton Conference on Communication, Control, and Computing, pp. 368-377, 1999.
\bibitem{vib} 
A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy
\textit{Deep variational information bottleneck}.
 arXiv:1612.00410, 2016.
 \bibitem{kingma} 
 Diederik P Kingma and Max Welling.
 \textit{Auto-encoding variational Bayes.}
  ICLR, 2014
 \bibitem{gru} 
 Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio.
 \textit{Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.}
  NIPS, 2014
 \bibitem{graves}
 Graves, Alex.
 \textit{Generating sequences with recurrent neural networks.}
 CoRR, abs/1308.0850, 2013
\bibitem{s2s}
Ilya Sutskever, Oriol Vinyals and Quoc V. Le
\textit{Sequence to Sequence Learning with Neural Networks}
NIPS 2014
\bibitem{attention}
Dzmitry Bahdanau, KyungHyun Cho and Yoshua Bengio
\textit{Neural Machine Translation by Jointly Learning to Align and Translate}
ICLR 2015
\bibitem{vae}
Diederick P. Kingman and Max Welling
\textit{Auto-encoding variational Bayes}
ICLR, 2015
\end{thebibliography}
\end{document}
