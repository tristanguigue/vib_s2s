\documentclass[11pt,oneside,openright]{report} 

\title{Sequence to Sequence learning with the Variational Information Bottleneck}
\author{Tristan Guigue}
\date{2017}

\usepackage[mastersc]{edmaths}
\usepackage[parfill]{parskip}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{float}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{placeins}
\usepackage{pgfplots}
\usetikzlibrary{bayesnet}
\usepackage[font=small,labelfont=bf]{caption}
\usetikzlibrary{fit, positioning, arrows.meta}
\usepackage[makeroom]{cancel}
\tikzset{
neuron/.style={shape=circle, minimum size=1.1cm,  inner sep=0, draw, font=\small}, io/.style={neuron, fill=gray!20}, deterministic/.style={diamond, minimum size=1.3cm, draw, text badly centered, inner sep=3pt}}
\newtheorem{notation}{Notation}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\DeclareMathOperator{\sign}{sign}

\begin{document}

\maketitle

\declaration

\begin{abstract}
Regularisation is a challenging task in a neural network context. As neural networks are very expressive,  promoting simpler and more robust mappings can be difficult. Previous approaches have made used of dropping randomly units from the network to prevent complex co-adaptations on the training data.

In this dissertation we will focus on another regularisation technique: the information bottleneck method and apply it to the problem of predicting an output sequence from an input sequence. The encoder embeds the input sequence and the decoder generates the output sequence. Both can be implemented as recurrent neural networks. The output of the encoder is a vector of fixed size that contains all the information required to generate the output sequence. The sequence to sequence problem can then be formulated as finding a vector that is the best representation of the input in terms of its prediction capacity on the output sequence. The information flowing from the input to the output needs to pass through this vector which is therefore an information bottleneck.

The information bottleneck method formalises what it means to get the best tradeoff between compression of the input and accuracy. It can be formulated as finding a representation that is maximally compressive on the input while being maximally predictive on the output in term of their mutual information. The compression ensures that our mapping generalises well, acting as a regulariser. It has recently been shown \cite{vib} that the information bottleneck method is able to handle a large array of inputs and neural network mappings using variational approximations to get a lower bound on the original objective. In this dissertation we use this model to provide a regularisation method to the the sequence to sequence problem.

We use several practical examples to show the effectiveness of the information bottleneck method and show it performs similarly to using dropout regularisation. We notice that the information bottleneck is especially effective when dealing with high dimensional inputs and low dimensional outputs and that reducing the entropy of the input reduces its effect. We note that using attention mechanisms prevent dealing with this type of bottleneck but that features generated by the bottleneck vector could help better understand sequence to sequence mappings.  


\end{abstract}

\tableofcontents

%------------------------------------------------------------------------------------------------
\chapter{Introduction}

\section{Information Bottleneck Regularisation}
Supervised learning consist in inferring a mapping function between inputs and outputs from pairs of training example. A good mapping will generalise well to unseen data. If the mapping learned is too complex, the function will be able to map exactly each input to the right output, our training error will be close to zero. However this might not represent the true underlying relationship between the two random variables, we are overfitting. To prevent this, we have to promote simpler mappings using regularisation techniques such as the information bottleneck. 

The idea of the information bottleneck regularisation method introduced by Tishby et al. \cite{tishby}. is to pass the input through an information bottleneck such as to keep only the information that the input contains about the output. In this way we ensure that our mapping won't be affected by input noise and will reflect what needs to be learned about the input to predict the output. This is especially valid when dealing with high dimensional inputs that contain a lot of information not all relevant to the task.

\section{Motivations}
In sequence to sequence learning we try to map an input sequence to an output sequence. This type of problem can be solved using an encoder to find a representation of the input sequence and a decoder to generate the output sequence. In this case the representation of the input sequence has to contain all the information required to generate the output sequence, this makes it a good candidate to apply the information bottleneck method.

As the information bottleneck method does not have a solution for most inputs we will need to use variational approximations to find a lower bound on our objective and optimise that lower bound as recently shown by Alemi et al. \cite{vib}. In their work, they show that this technique outperforms other regularisation methods, we would like to confirm this in the sequence to sequence context.
 
\section{Objectives}
In this dissertation we hope to verify that using the information bottleneck regularisation does help getting better performance on sequence to sequence and sequence to label problems and study how it compares with other regularisation techniques. To be able to deal with sequences we would like to use recurrent neural networks as our encoder and decoder of the representation layer. We wish to compare the effect of changing our regularisation parameter and analyse how this affects the learning process. 

In particular we would like to apply this model to various setup such as predicting a sequence of pixel to complete an image or finding a list of labels corresponding to a sequence of images.

\section{Structure}

In the second chapter we go through a review of the information bottleneck method, the variational inference framework, introduce recurrent neural networks and the sequence to sequence problem as well as dropout regularisation.

In the third chapter we analyse the maximisation of mutual information in different models and compare it to the maximum likelihood to get a better understanding of information maximisation.

In the fourth chapter we present the model we used, derivate the objective function and find a lower bound to optimise. We then find an estimator for this lower bound and explain how to perform gradient descent on the estimator. Finally we discuss implementation considerations.

In the fifth chapter we explain the experiments that we have setup and present the results in terms of optimal loss or accuracies obtained.

Finally in the conclusion we discuss those results, go through the limitations we encountered and propose further work.

%------------------------------------------------------------------------------------------------
\chapter{Background and Literature Review}
\section{Latent Generative Models and Variational Approximations}
\subsection{Likelihood in Latent Variable Models}
\begin{notation}
Stochastic nodes are represented as circles and deterministic nodes are represented as diamonds. Node of observed data are filled in grey.
\end{notation}

\begin{figure}[H]
\centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
 \node [neuron] (z) {$Z$};
\node [io, below=of z] (x) {$X$};
\draw [->] (z) -- (x);
\end{tikzpicture}
\caption{A simple generative model with a latent variable $Z$}
\end{figure}

A generative model provides assumptions in how observed data was generated. This generation process is assumed to be stochastic in nature, the observed data represented by a random variable $X$ depending stochastically on other random variables. Hidden variables or latents can be introduced to better model the structure believed to have generated the data and include hierarchy. The latents provide compression and representation of the data. They can themselves assume to be generated by observed variables and be used to predict other variables as we will see in the information bottleneck method. 

We define the likelihood of a set of observed data points $x$ under a model with a set of parameters $\theta$ as 
$$\mathcal{L}(\theta|x) = p(x|\theta)$$

For a model with latent variables, those need to be integrated out to find the likelihood:

\begin{equation}
p(x|\theta) = \int p(x|z, \theta) p(z|\theta) dz
\label{eq:lik}
\end{equation}

\subsection{Variational Inference}
The integral introduced in equation \ref{eq:lik} is intractable in general. To solve this issue we need to approximate the posterior distribution on the latent $p(z|x)$ by a function we can easily sample from and that we call $q_\phi(z|x)$, the parameters $\phi$ of this function are the variational parameters. We can rewrite the integral as:

\begin{align}
p(x|\theta) &= \int p_\theta(x|z) p_\theta(z) \frac{q_\phi(z|x)}{q_\phi(z|x)}dz \\
\end{align}

\begin{theorem}[Jensen's Inequality]
Let f be a concave function, then
$$ f(\mathbb{E}[x]) \geq \mathbb{E}[f(x)] $$
\end{theorem}

As the $\log$ is concave we can use Jensen's inequality to find a lower bound for the log likelihood. We note that since the $\log$ is also monotonically increasing finding a maximium for the log likelihood is equivalent to finding a maximum for the likelihood.

\begin{align}
\log p(x|\theta) &= \log \int q_\phi(z|x) p_\theta(x|z) \frac{p_\theta(z)}{q_\phi(z|x)}dz\\
	&\geq  \int q_\phi(z|x) \log \Big[  p_\theta(x|z) \frac{p_\theta(z)}{q_\phi(z|x)} \Big]dz\\
	&=  \mathbb{E}_q[\log  p_\theta(x|z)] - \int q_\phi(z|x) \log   \frac{q_\phi(z|x)}{p_\theta(z)}dz
\label{eq:elbo}
\end{align}

This is the evidence lower bound that can then be optimised by tightening the bound to get as close as possible
to the true marginal likelihood. The solution found is a good approximation to the maximum likelihood if the approximated distribution is close to the true posterior. This way we have turned a problem of integration into one of optimisation. In this dissertation we will use the variational inference principle but apply them to find lower bounds on mutual information rather than likelihood.

\subsection{Sampling}
Another way to overcome the intractability of an expectation is to generate random samples and approximate the results by the mean of those samples. That estimator we get is unbiased.

\subsubsection{Monte Carlo Sampling}
If it is possible to sample directly from the probability $p$ under which we take the expectation then we can use Monte Carlo Sampling.

\begin{theorem}[Law of large numbers]
If $X_i$ is a collection of independent identically distributed random variables with density $p(x)$, then
$$ \lim_{N \to \infty} \frac{1}{N} \sum_i f(X_i) = \int f(x) p(x) dx $$
\end{theorem}

In Monte Carlo sampling we use the law of large number to introduce an estimate  of the expectation:
Let $$\theta = \mathbb{E}_{x \sim p(x)}[f(X)]$$

Then by the law of large number 

$$ \hat{\theta} =  \frac{1}{N} \sum_i f(X_i) $$ is an unbiased estimate of $\theta$.

\section{The Information Bottleneck}
\subsection{Regularisation in Supervised Learning}
In supervised learning, we want to infer a random variable $Y$ from an input given by the random variable $X$, learning from labelled training data. We assume here that $Y$ is not independent from the input $X$. The objective is to learn a mapping from $X$ to $Y$ that generalises well to unseen data. To ensure the learning process learns the underlying relationship between $X$ and $Y$ rather than noise in the training data, there are different regularisation techniques possible.

One form of regularisation was introduced by Tishby et al. \cite{tishby}. For each input they seek a stochastic mapping to a representation of the input that provides the most relevant information about the output. They used the concept of mutual information to formalise what a good representation of the input should be extracted. 

The objective is then to learn a representation $Z$ that is maximally compressive on $X$ while being maximally informative about the target $Y$. We want to squeeze the information that $X$ contains about $Y$ trough a bottleneck and keep only the most meaningful information about the output. For example when learning the transcript of words from acoustic data, it is possible to greatly compress the input data while still perserving enough to predict the words as the transcript has a much smaller entropy than the acoustic data.

\subsection{Mutual Information}

\begin{definition}
The amount of information that $Z$ contains about $Y$ is given by:
$$ I(Z, Y) = \int p(y, z) \log \frac{p(z, y)}{p(z)p(y)} dy\,dz $$ where Y and Z are continous random variables or 
 $$ I(Z, Y) = \sum_y \sum_z p(y, z) \log \frac{p(z, y)}{p(z)p(y)} $$ where Y and Z are discrete random variables 
\end{definition}

\begin{definition}
The KL divergence from a continuous probability distribution $p$ to another continuous probability distribution $q$ is defined as 
$$ KL[p(x)|q(x)] = \int p(x) \log \frac{p(x)}{q(x)} dx $$
\end{definition}

\begin{corollary}
The mutual information of two random variable is equal to the Kullback-Leibler divergence from the joint distribution of the random variables to the product of the distributions.
$$ I(Z, Y) = KL[p(z, y) | p(z)p(y)] $$
\end{corollary}
The mutual information therefore measures how the joint distribution differs from the product of the marginal distributions. 

\begin{definition}
The entropy of a continuous random variable $Y$ is given by:
$$ H(Y)  = -\int p(y) \log p(y) dy $$
\end{definition}

\begin{definition}
The conditional entropy of a continuous random variable $Y$ given another continuous random variable $Z$ is given by:
$$ H(Y|Z)  = -\int p(y, z) \log p(y|z) dy\,dz $$
\end{definition}
Which can be understood as the how much uncertainty is left in $Y$ after observing $Z$.

\begin{corollary}
Let $Y$ and $Z$ be random variables:
\begin{equation}
I(Z, Y) = H(Y) - H(Y|Z)
\label{eq:mutual_info_cond_entropy}
\end{equation}
\end{corollary}

\begin{proof}
\begin{align}
I(Z, Y) &= \int p(y, z) \log \frac{p(z, y)}{p(z)p(y)} dy\,dz\\
& = \int p(y, z) \log \frac{p(y|z)}{p(y)} dy\,dz \label{eq:mi_zy}\\
&= \int p(y, z) \log p(y|z) dy\,dz - \int p(y, z) \log p(y) dy\,dz\\ 
&= - H(Y|Z) - \int p(y) \log p(y) dy\\ 
&= H(Y) - H(Y|Z)
\end{align}
\end{proof}

Hence when $Y$ and $Z$ are independent then  $H(Y|Z) = H(Y)$ and $I(Z, Y) = 0$.  And if $Y$ is a deterministic function of $Z$ then $H(Y|Z) = 0$ and $I(Z, Y) = H(Y)$. Maximizing the mutual information $I(Z, Y)$ with $H(Y)$ fixed as it will be the case here is equivalent to minimizing the conditional entropy $H(Y|Z)$.

\subsection{Graphical Model}

\begin{figure}[H]
\centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
 \node [io] (x) {$X$};
\node [neuron] at (-1, -0.8) (z) {$Z$};
\node [io, below=of x] (y) {$Y$};
\draw [->] (x) -- (z);
\draw [->] (x) -- (y);
\end{tikzpicture}
\caption{Graphical model of the information bottleneck}
\end{figure}

In the information bottleneck model, both the output and the representation depends solely on the input, that is we assume $p(z | x, y) = p(z|x)$. $Z$ is not part of the generative process of $Y$ as it would if $Z$ would be considered a stochastic layer between $X$ and $Y$. In this case $Z$ is only there to ensure we learn a useful representation of $X$ that will help us to predict $Y$.

The factorisation theorem lets us express the joint distribution as:
$$ p(x, y, z) = p(x)\, p(y|x)\, p(z|x)$$

\subsection{Objective function}
In this section we will assume that the random variables are continuous. We define $p_\theta(z|x)$ as our stochastic encoding of the input where $\theta$ are the model parameters. The mutual information of the representation $Z$ and the target $Y$ can then be expressed as:

$$ I(Z, Y|\theta) = \int p_\theta(y, z) \log \frac{p_\theta(z, y)}{p_\theta(z)p(y)} dy\,dz $$

The objective of the information bottleneck method is to maximise $I(Z, Y|\theta)$ while constraining how much information is shared between $X$ and $Z$ such as to force $Z$ to "forget" $X$ and act as a minimal sufficient statistic of $X$ for predicting $Y$. The quantity we are constraining is therefore the mutual information of the input $X$ and its representation $Z$:

$$ I(X, Z|\theta) = \int p_\theta(x, z) \log \frac{p_\theta(x, z)}{p(x)p_\theta(z)} dy\,dz $$

We set $$I(X, Z) < I_c$$ where $I_c$ is the information constraint. This constraint can be expressed using a Lagrange multiplier such as to write the whole objective function as:

$$ J_{IB}(\theta) = I(Z, Y|\theta) - \beta I(X, Z|\theta)$$


\subsection{Iterative Solution}
\begin{theorem}
The information bottleneck objective function has an exact formal solution when X, Y and Z are all discrete:
$$ p(z|x) = \frac{p(z)}{Z(x, \beta)} \exp\Big[-\beta \sum_y p(y|x) \log\frac{p(y|x)}{y|z}\Big] $$
\end{theorem}

The proof can be found in \cite{tishby}. The expression $p(y|z)$ can be expressed as 
 
 \begin{align}
p(y|z) &= \sum_x p(y, x|z)  \\
	 & = \sum_x \frac{p(x ,y, z)}{p(z)} \\
	 &= \frac{1}{p(z)} \sum_x p(x) p(y|x) p (z|x) 
\label{eq:y_given_z}
\end{align}

And $p(z)$ can be expressed as a mixture:
 
  \begin{align}
p(z) &= \sum_x p(z, x)  \\
	 &= \sum_x p(x) p(z|x)
\label{eq:z_post}
\end{align}

In the formal solution, the encoder is present on both sides, Tishby et al. present a scheme to find a optimum by iteratively updating $p(z|x)$, $p(y|z)$ and $p(z)$.

\subsection{Relation to Maximum Likelihood}
The information bottleneck problem and the maximum likelihood problem stem from different motivations however in \cite{ib_ml}, Slonim et al. shows that the information bottleneck is strongly related to maximum likelihood of mixture models and equivalent in the large sample size limit. However the mixture model does not have the same markovian independence assumptions. In chapter 3 we look at maximum likelihood and mutual information in that model.

\section{Recurrent Neural Networks}

\subsection{Architecture}
A neural network is a composition of linear and non-linear transformations organised in layers. It is a deterministic function of the input. The output of the neural network is given by:

$$ \hat{y} = f(x | W) = \sigma_L (W_L h_{L-1}(x)) $$

Where:
\begin{itemize}
\item $\hat{y}$ is the predicted output
\item $\sigma$ is a non-linear function
\item $W$ is the set of weights of the linear transformations
\item $L$ is the index of the output layer
\item $h_l$ are the hidden layers defined as:
  \begin{equation}
    \begin{cases}
          h_1(x) = \sigma_1(W_1 x)\\
          h_l(x) = \sigma_l (W_l h_{l-1}(x)) \quad \forall l \in \{2, L\}\\
    \end{cases}
  \end{equation}
\end{itemize}

\begin{figure}[H]
\centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
 \node [io] (x) {$X$};
\node [deterministic, right=of x] (h1) {$h1$};
\node [deterministic, right=of h1] (h2) {$h2$};
\node [right=of h2](dots) {\ldots};
\node [deterministic, right=of dots] (hL) {$h_L$};
 \node [io, right=of hL] (y) {$Y$};
\draw [->] (x) -- (h1);
\draw [->] (h1) -- (h2);
\draw [->] (h2) -- (dots);
\draw [->] (dots) -- (hL);
\draw [->] (hL) -- (y);
\end{tikzpicture}
\caption{Graphical model of a neural network}
\end{figure}

In a recurrent neural network as introduced by Graves et al \cite{graves}, an input is given at each time step so the data is processed sequentially, this is especially useful to model language, music, stock prices, etc.

In that case the hidden states at each layer are a linear combination of the input and the previous hidden state such as to carry information from the past:
  \begin{equation}
    \begin{cases}
          h_1(x_1) = \sigma_1(W x_1) \\
          h_l(x_{1:l}) = \sigma_l (U h_{l-1}(x_{1:l-1}) + W x_l) \quad \forall l \in \{2, L\}\\
    \end{cases}
  \end{equation}

We use the same set of weights: U, W at each step since we are performing the same task with different inputs, this makes the parameter space much smaller.

\subsection{Supervised learning settings}
When dealing with sequential data, there are several possible supervised learning problems:
\subsubsection{Many to many}
\begin{figure}[H]
\centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
\foreach \jlabel [count=\j, evaluate={\jj=int(\j-1); \jd=int(2 * \j);}]  in {l-1,  l, l+1}{
      \node [deterministic] at (\jd, 1) (h-\j) {$h_{\jlabel}$};
      \node [io, above=of h-\j] (x-\j) {$X_{\jlabel}$};
      \node [io, below=of h-\j] (y-\j) {$Y_{\jlabel}$};
      \draw [->] (x-\j) -- (h-\j);
      \draw [->] (h-\j) -- (y-\j);
      \ifnum\j>1
          \draw [->] (h-\jj.east) -- (h-\j.west);
      \fi
} 
\node [left=of h-1] (ldots){\ldots};
\node [right=of h-3] (rdots){\ldots};
\draw [->] (h-3) -- (rdots);
 \draw [->] (ldots) -- (h-1);
\end{tikzpicture}
\caption{Many to many recurrent neural network graphical model}
\end{figure}
In the many to many problem we try to predict the output at each time step based on previous inputs. For example, we process an image sequentially and try to predict the next pixel at each step, in which case the targets are the next inputs.

In this setting the output is given at each time step by:
$$ \hat{y_l} = f(x_{1:l}|W, U, V) = V h_l(x_{1:l}|W,U)\quad \forall l \in \{1, L\}$$

\subsubsection{Many to one}
\begin{figure}[H]
\centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
\foreach \jlabel [count=\j, evaluate={\jj=int(\j-1); \jd=int(2 * \j);}]  in {1,  2}{
      \node [deterministic] at (\jd, 1) (h-\j) {$h_{\jlabel}$};
      \node [io, above=of h-\j] (x-\j) {$X_{\jlabel}$};
      \draw [->] (x-\j) -- (h-\j);
      \ifnum\j>1
          \draw [->] (h-\jj.east) -- (h-\j.west);
      \fi
} 
\node [right=of h-2] (rdots){\ldots};
\draw [->] (h-2) -- (rdots);
\node [io, above=of h-3] (x-3) {$X_L$};
\node [deterministic, right=of rdots] (h-L) {$h_L$};
\draw [->] (rdots) -- (h-3);
\draw [->] (x-3) -- (h-3);
\node [io, below=of h-3] (y) {$Y$};
\draw [->] (h-L) -- (y);
\end{tikzpicture}
\caption{Many to one recurrent neural network graphical model }
\end{figure}

In the many to one problem, we try to predict the output after having processed a whole sequence. For example we try to predict the label of an image after sequentially processing all the pixels in the image.

In this case the predicted output is given by:
$$ \hat{y} = f(x_{1:L}|W, U, V) = V h_L(x_{1:L}|W,U)$$

\subsubsection{Sequence to sequence}

Sequence to sequence learning aims at predicting an output sequence from an input sequence. Using neural network to solve this was first used by Sutskever et. al. \cite{s2s}. Translation of a sentence is an example of a sequence to sequence task. The output sequence does not necessarily have the same length as the input sequence.


\begin{figure}[H]
\centering
\begin{tikzpicture}[x=0.95cm, y=0.6cm]
\foreach \jlabel [count=\j, evaluate={\jj=int(\j-1); \jd=int(2 * \j);}]  in {1,  2}{
      \node [deterministic] at (\jd, 1) (h-\j) {$h_{\jlabel}$};
      \node [io, above=of h-\j] (x-\j) {$X_{\jlabel}$};
      \draw [->] (x-\j) -- (h-\j);
      \ifnum\j>1
          \draw [->] (h-\jj.east) -- (h-\j.west);
      \fi
} 
\node [right=0.5cm of h-2] (rdots){\ldots};
\draw [->] (h-2) -- (rdots);
\node [deterministic, right=0.5cm of rdots] (h-L) {$h_L$};
\node [io, above=of h-L] (x-L) {$X_L$};
\draw [->] (rdots) -- (h-L);
\draw [->] (x-L) -- (h-L);

\foreach \jlabel [count=\j, evaluate={\jj=int(\j-1); \jd=int(2 * \j);}]  in {1, 2}{
      \node [deterministic] at (8 + \jd, 1) (hpred-\j) {$i_{\jlabel}$};
      \node [io, below=of hpred-\j] (ypred-\j) {$Y_{\jlabel}$};
      \draw [->] (hpred-\j) -- (ypred-\j);
      \ifnum\j>1
          \draw [->] (hpred-\jj.east) -- (hpred-\j.west);
      \fi
} 
\draw [->] (h-L) -- (hpred-1);
\node [right=0.5cm of hpred-2] (rdots){\ldots};
\draw [->] (hpred-2) -- (rdots);
\node [deterministic, right=0.5cm of rdots] (hpred-M) {$i_M$};
\node [io, below=of hpred-M] (ypred-M) {$Y_M$};
\draw [->] (rdots) -- (hpred-M);
\draw [->] (hpred-M) -- (ypred-M);
\end{tikzpicture}
\caption{Sequence to sequence graphical model}
\end{figure}

The output predictions are given with respect to the decoder hidden states:
\begin{equation}
          \hat{y}_{m} = V i_{m} \quad \forall m \in \{1, M\}\\
\end{equation}

\section{Optimisation}
We introduce a loss function of the predicted output and the true values that we intend to minimise.

$$L(x, y, W) = \sum_{i=1}^N l(y_i, \hat{y_i} = f(x_i|W))$$

Where $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N$ is our training data. 

In this dissertation we will use the cross-entropy loss for binary data:

$$ L(\hat{y}, y) = y \log \hat{y} + (1-y) \log(1 - \hat{y}) $$

And its equivalent for categorical data with K classes:

$$ L(\hat{y}, y) = \sum_{i=1}^K y_i \log \hat{y}_i $$

Where $y_i$ is a one-hot encoding of the category y.

\subsection{Gradient Descent}
In most cases a close-form solution to minimising L is not available so we use gradient descent to improve the parameters iteratively.

For a given parameter $w$:

$$ w_{k+1} = w_{k} - \eta \nabla L(x, y, w)$$

We can show that this always decreases $L$ if $\eta$ is small enough. By the first order Taylor expansion:

$$L(w_{k+1}) \approx L(w_{k}) + (w_{k+1} - w_{k})^T  \nabla L_{w}(w_{k}) $$

We get:

$$L(w_{k+1}) \approx L(w_{k}) - \eta | \nabla L(w_{k})|^2 $$

We can iterate this procedure until we reach convergence. However using gradient descent can get us stuck into local optima or saddle points. In this dissertation we use the Adam optimiser \cite{adam} that take advantage of exponential moving averages of the first and second moments of the gradients to improve the optimisation.

 \subsection{Backpropagation}
To obtain the gradient with respect to each parameter $W_l$ in a neural network we use the chain rule to backpropagate the error through the network layers.
 
 \begin{align}
   \nabla_{W_l} L(y, x, W) &= \frac{\partial L}{\partial h_{L-1}} \nabla_{W_l} h_{L-1}\\
   &= \frac{\partial L}{\partial h_{L-1}} \frac{\partial h_{L-1}}{\partial h_{L-2}} \nabla_{W_l} h_{L-2}\\
   &...\\
   &= \frac{\partial L}{\partial h_{L-1}} \frac{\partial h_{L-1}}{\partial h_{L-2}} ... \frac{\partial h_l}{\partial W_l}
 \end{align}
 This is a special case of the reverse mode automatic differenciation and can be done in a computational effective way by storing the required gradients.

\subsection{Vanishing and Exploding Gradients}
In the recurrent neural network setting, our network can be very deep as every input adds a layer to the network. To adjust the weights based on the first input, the error signal is backpropagated from the loss layer all the way to the first layer: 

 \begin{align}
   \nabla_{W_1} L(y, x, W) = \frac{\partial L}{\partial h_{L-1}} \frac{\partial h_{L-1}}{\partial h_{L-2}} ... \frac{\partial h_1}{\partial W_1}
 \end{align}
 
 If we ignore the non-linearity we have:
 $$ \frac{\partial h_{l}}{\partial h_{l-1}} = U $$
 
 And 
 
 $$ \nabla_{W_1} L(y, x, W) = U^{L-1}$$
 
 If we take the univariate example, having $U$ smaller than one would give a very small gradient, therefore the changes in the parameters of the first layer would be very small and we would have difficulties learning. Vice-versa if $U$ is larger than one, the gradient will be very large and this will lead to instabilities that will prevent learning the optimal weights. In the multivariate case the gradient will explode if the largest eigenvalue of U is larger than one, and vanish if the largest eigenvalue is smaller than one.

\subsection{Gated Recurrent Unit}
To overcome the vanishing or exploding gradient problem, different architectures were proposed including the Gated Recurrent Unit \cite{gru}. Instead of taking $h_l = \sigma(U h_l + W x_l)$ at each step they introduce a gate mechanism that allows a better control of the information that flows through the network.

The operations performed at each step are the following:
\begin{align}
z_t &= \sigma_g(W_{z} x_t + U_{z} h_{t-1} + b_z) \\
r_t &= \sigma_g(W_{r} x_t + U_{r} h_{t-1} + b_r) \\
h_t &=  z_t \circ h_{t-1} + (1-z_t) \circ \sigma_h(W_{h} x_t + U_{h} (r_t \circ h_{t-1}) + b_h)
\end{align}
Where:
\begin{itemize}
 \item $z_t$ is the update gate that defines how much we keep from the previous state
 \item $r_t$ is the reset gate that determines how to combine the new input with the previous state
\item  $\sigma_g$ is the sigmoid function
\item $\sigma_h$ is the hyperbolic tangent
\end{itemize}

One of the main advantage is that by simply setting the update gate to one, one can learn the identity function. This makes it easier to preserve information from the past.


\section{Regularisation in Neural Networks}
Deep neural network are very expressive models that can handle a lot of complexity. A neural network will typically overfit after enough learning steps given limited training data. One way to deal with this is to use early-stoping: to stop the learning process before the model become too complex and before the error on our validation set start to increase. However a better way is to reduce the overfitting all-together by preventing the model to learn overly complex mappings.

\subsection{Dropout}

\begin{center}
\includegraphics[scale=0.6]{dropout}
\captionof{figure}{Dropout Neural Network Model. Left: A standard neural net with 2 hidden layers. Right: An example of a thinned net produced by applying dropout to the network on the left. Crossed units have been dropped. From \cite{dropout}}
\end{center}

In this dissertation we will compare the Information Bottleneck regularisation with another kind of regularisation widely used in neural networks : dropout. The key idea is to randomly and temporarily remove units from the neural network such as to prevent units to co-adapt (Srivastava et al. \cite{dropout}). Each unit is removed with a fixed probability $p$. This way almost every model trained is different while sharing a great number of weights. We therefore combine the predictions of an exponential number of 'thinned' models. At test time the whole network is used with the weights scaled down by the probability of dropout so that the expected output stays the same.

In the recurrent neural network context, recent work by Gal et al. \cite{gal} has shown using a bayesian interpretation of the parameters that the same network units should be dropped at each time step for input, output as well as recurrent connections. We use this technique in our dissertation.

%------------------------------------------------------------------------------------------------
\chapter{Comparison of Supervised Learning Model}

In this chapter we study the relationship between mutual information maximisation and maximum likelihood in different models.

\section{Fully observed model}

We consider the graphical model of a supervised learning problem where the input is given by the random variable $X$ and the output by the random variable $Y$:

\begin{figure}[H]
\centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
 \node [io] (x) {$X$};
\node [io, below=of x] (y) {$Y$};
\draw [->] (x) -- (y);
\end{tikzpicture}
\end{figure}

We define $p(y|x, \theta)$ as the distribution of the output given the input and our model parameters $\theta$.

The mutual information of $X$ and $Y$ is given by \ref{eq:mutual_info_cond_entropy}:

\begin{align}
I(X, Y) &= H(Y) - H(Y|X)
\end{align}

The entropy of the labels $H(Y)$ is independent of our model parameters $\theta$ and will therefore be ignored when taking the gradient with respect to the parameters:

$$ \nabla_\theta I(X,Y) = -\nabla_\theta H(Y|X)$$ 

Using the empirical joint distribution 

$$p(x, y) \approx \frac{1}{N}\sum_n\delta_{x_n}(x)\delta_{y_n}(y)$$

We can approximate the conditional entropy:

\begin{align}
- H(Y|X) &= \int p(x, y) \log p_\theta(y|x) dx\, dy\\
	      &\approx \int \frac{1}{N}\sum_n\delta_{x_n}(x)\delta_{y_n}(y) \log p_\theta(y|x)dx\, dy\\
	      &= \frac{1}{N} \sum_n \log p_\theta(y_n|x_n,)
\end{align}

The log likelihood is given by:
\begin{align}
 l[p_\theta(y|x)] &= \log \prod_n p_\theta(y_n|x_n)\\
 	      &= \sum_n \log p_\theta(y_n|x_n)
\end{align}

We can conclude that mutual information maximisation under an empirical distribution is equivalent to maximum likelihood in this fully observed supervised model:

\begin{align}
\nabla_\theta I(X,Y) = 0 \Leftrightarrow \nabla_\theta l[p_\theta(y|x)] = 0
\end{align}

\section{Stochastic Feed-forward Network}

We now consider a stochastic feed-forward network model that contains an encoder $p_\theta(z|x)$ and a decoder $p_\theta(y|z)$. This is different from the information bottleneck setting as in this case $Z$ is a latent variable part of the generative process of the outputs rather than a representation of the input. This can be useful to introduce depth in the generating process and add stochasticity in a neural network. 

\begin{figure}[H]
\centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
 \node [io] (x) {$X$};
\node [neuron, below=of x] (z) {$Z$};
\node [io, below=of z] (y) {$Y$};
\draw [->] (x) -- (z);
\draw [->] (z) -- (y);
\end{tikzpicture}
\end{figure}

\subsection{Maximum Likelihood}

To get the maximum likelihood we can simply use a Monte Carlo approximation to the expectation of the decoder under the encoder distribution:

\begin{align}
 p(y|x) &= \int p_\theta(y, z|x) dz \\
 &= \int p_\theta(y|z) p_\theta(z|x) dz \\
 &= E_{Z | X}[p_\theta(y|z)] 
\end{align}

Which becomes:
\begin{align}
 p_\theta(y|x) &\approx \frac{1}{M} \sum^M_{m=1} p(y | z^{(m)}) 
\end{align}

Where $ z^{(m)} \sim p_\theta(z|x) $

However this estimator has a high variance in high dimensional space because our draw from $p_\theta(z | x)$ might not contribute significantly to our estimate $p(y |x)$. For most $z$, $p_\theta(y|z)$ will be nearly zero. We would need an very large number of samples to ensure that we will generate samples that contribute to $p(y|x)$, this makes it impractical.

\subsubsection{Variational Bayes}

To solve this, we would like to sample values of $Z$ that are likely to have produced $Y$. To do so we introduce a variational approximation $q_\phi(z|x, y)$ of $p(z|x, y)$ and optimise the variational lower bound with respect to the variational parameters $\phi$.
 
\begin{align}
 \log p(y|x) &= \log(\int p_\theta(y, z|x) dz) \\
 & =  \log(\int q_\phi(z|x, y) \frac{p_\theta(y, z|x)}{q_\phi(z|x, y)}) \\
 & \geq \int q_\phi(z|x, y) \log \frac{p_\theta(y, z|x)}{q_\phi(z|x, y)}
\end{align}

Using Jensen's inequality.

\begin{align}
 \log p(y|x) & \geq \int q_\phi(z|x, y) \log \frac{p_\theta(z|x) p_\theta(y| z)}{q_\phi(z|x, y)} \\
  &= \int q_\phi(z|x, y) \log \frac{p_\theta(z|x)}{q_\phi(z|x, y)}  + \int q_\phi(z|x, y) \log p_\theta(y|z) \\
  &= E_q[p_\theta(y|z)] - KL\big[q_\phi(z|x, y) || p_\theta(z|x)]\\
  &\approx  \frac{1}{M} \sum^M_{m=1} p_\theta(y | z^{(m)}) - KL\big[q_\phi(z|x, y) || p_\theta(z|x)\big]
\end{align}
Where $ z^{(m)} \sim q_\phi(z|x, y)$, using a Monte Carlo approximation of the expectation and assuming the KL divergence can be computed analytically.

The bound is exact when $q_\phi(z| x, y) = p(z|x, y)$. The first term corresponds to the reconstruction cost and the KL divergence between $q_\phi(z|x, y)$ and $p_\theta(z|x)$ acts as a regulariser. 

\subsection{Maximum Mutual Information}

We would like to compare this result to the maximisation of the  mutual information between Z and Y:

\begin{align}
I(Z, Y|\theta) &= \int p_\theta(y, z) \log \frac{p_\theta(z, y)}{p_\theta(z)p(y)} dy\,dz\\
& = \int p_\theta(y, z) \log \frac{p_\theta(y|z)}{p(y)} dy\,dz\\
&= \int p_\theta(y, z) \log p_\theta(y|z) dy\,dz - H(Y) 
\end{align}
We ignore the entropy of the labels as it is independent of our model parameters. We get:
\begin{align}
I(Z, Y|\theta) &= \int p_\theta(x, y, z) \log p_\theta(y| z) dx\,dy\,dz + C\\
&= \int p_\theta(z | x, y) p(x, y) \log p_\theta(y | z) dx\,dy\,dz + C
\end{align}
We can approximate the joint distribution of the data and the labels by its empirical distribution:
$$ p(x, y) \approx \frac{1}{N}\sum_n \delta_{x_n}(x) \delta_{y_n}(y)$$
And we get:
$$ I(Z, Y|\theta) \approx \frac{1}{N} \int p(z | x^{(n)}, y^{(n}) \log p_\theta(y^{(n)} | z) + C$$

As we can see this is not equivalent to the maximum likelihood results. In particular we have no simple way to approximate the posterior $p(z | x^{(n)}, y^{(n})$ to get a lower bound.


%------------------------------------------------------------------------------------------------
\chapter{Model}
\section{Deep Variational Information Bottleneck}

In the second chapter a solution to the information bottleneck problem was presented when X, Y and Z are all discrete. However in the general case there won't be a formal solution or an iterative method to find the parametric encoder. Alemi at al. in \cite{vib} introduce a variational approach to the information bottleneck. They use variational inference to provide a lower bound to the information bottleneck objective. This allow us to use stochastic gradient ascent to find an optimum of the lower bound and to parametrise the encoder with a neural network which can handle a broad range of data.

\subsection{Variational Lower Bound}

\subsubsection{Lower bound on $I(Z, Y)$}
The equation \ref{eq:y_given_z} is not tractable in the continuous case. Indeed
\begin{align}
p_\theta(y|z) &= \frac{1}{p(z)} \int p(x, y) p_\theta(z|x) dx
\end{align}

Using $q_\phi(y|z)$ as a variational approximation to $p(y|z)$ we can find a lower bound to the mutual information of $Y$ and $Z$ as proposed in Barber et al. \cite{barber}.

\begin{lemma}
A lower bound to the mutual information of $Y$ and $Z$ is:
$$ \mathcal{L}^{A}(x, y, \theta, \phi) = \int p(x)\, p(y|x)\, p_\theta(z|x) \log q_\phi(y|z) dy\, dz\, dx + H(Y)$$
\end{lemma}

\begin{proof} In \ref{appendix:xz}\end{proof}
Note that since the entropy of the label is a constant, it will be ignored in our objective function.

If $q_\phi(z|y) = p(z|y)$, the bound is exact. When optimising the lower bound we will increase $I(Z, Y)$ while minimising $KL\big[q_\phi(z|y)|p(z|y)\big]$ which mean our approximation will get closer to the true posterior.

\subsubsection{Upper bound on $I(X, Z)$}
The marginal on $Z$ given in equation \ref{eq:z_post} is intractable as well:

$$ p(z) = \int p_\theta(z|x) p(z) dx $$

We approximate $p(z)$ by $r_\psi(z)$ which gives us the second part of the objective function. 

\begin{lemma}
An upper bound to the mutual information of $X$ and $Z$ is:
$$ \mathcal{U}(x, y, \theta, \psi) = \int p(x)p(y|x)p_\theta(z|x) \log \frac{p_\theta(z|x)}{r_\psi(z)}dx\, dy\, dz$$
\end{lemma}

\begin{proof} In \ref{appendix:zy}\end{proof}

Combining both bounds we get the following:
\begin{theorem}
A lower bound to the information bottleneck objective function can be written as:
 \begin{align}
\mathcal{L}(x, y, \theta, \phi, \psi) = \int p(x) p(y|x) p_\theta(z|x) \Big[ \log q_\phi(y|z) - \beta  \log \frac{p_\theta(z|x)}{r_\psi(z)}\Big] dx\, dy\, dz
\end{align}
\end{theorem}

One possibility to find an optimum to this problem would be to update iteratively the model parameter $\theta$ from $p_\theta(z|x)$  and the variational parameters $\phi$ and $\psi$ from $q_\phi(y|z)$ and $r_\psi(z)$ in a procedure similar to the variational expectation maximisation algorithm \cite{vem} for the maximum likelihood. Instead we will use a variational bayes inference method to learn together both the model and variational parameters and use stochastic gradient descent to optimise the bound.

We also note that the tightness of the bound depends on both variational approximations made.

\subsection{Estimating the lower bound}
Since X and Y are known during training, we can use the empirical data distribution $\tilde{p}(x, y) = \frac{1}{N}\sum_n\delta_{x_n}(x)\delta_{y_n}(y)$ to approximate the lower bound:

 \begin{align}
\tilde{\mathcal{L}} & = \int \tilde{p}(x, y) p_\theta(z|x) \Big[\log q_\phi(y|z) - \beta \log \frac{p_\theta(z|x)}{r_\psi(z)} \Big]dx\, dy\, dz  \\
   & = \int \frac{1}{N}\sum_n\delta_{x_n}(x)\delta_{y_n}(y) p_\theta(z|x) \Big[\log q_\phi(y|z) - \beta \log \frac{p_\theta(z|x)}{r_\psi(z)} \Big] dx\, dy\, dz \\
   & = \frac{1}{N}\sum_n \int \delta_{x_n}(x)\delta_{y_n}(y) p_\theta(z|x) \Big[ \log q_\phi(y|z) - \beta \log \frac{p_\theta(z|x)}{r_\psi(z)}\Big ] dx\, dy\, dz \\
   & = \frac{1}{N}\sum_n \int p_\theta(z|x_n) \Big[  \log q_\phi(y_n|z_n) - \beta   \log \frac{p_\theta(z|x_n)}{r_\psi(z)} \Big] dz \label{eq:lower}
\end{align}


\subsection{Reparametrisation Trick}
To apply gradient descent we would like to get the gradient with respect to the parameters for the lower bound estimation \ref{eq:lower}. However the first part of the estimator is problematic. Indeed we are taking the gradient of the parameters under which the expectation is taken: $\theta$.

One way to solve this would be using the score function:
 \begin{align}
\nabla_\theta \mathbb{E}_{p_\theta(z|x)}\big[ g(z) \big] &= \int_z \nabla_\theta p_\theta(z|x)  g(z)\\
 &= \int_z \frac{p_\theta(z|x)}{p_\theta(z|x)} \nabla_\theta p_\theta(z|x)  g(z) \\
 &= \int_z p_\theta(z|x) \nabla_\theta \log p_\theta(z|x)  g(z)\\
&= \mathbb{E}_{p_\theta(z|x)}\big[ \nabla_\theta \log p_\theta(z|x) g(z)  \big]\\
&\approx \frac{1}{L} \sum_l  g(z^{(l)}) \nabla_\theta \log p_\theta(z^{(l)}|x)
\end{align}

Where $z^{(l)} \sim p_\theta(z|x)$.

However it has been shown \cite{score} that this gradient estimator exhibits very high variance and is therefore impractical.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
 \node [neuron] (z) {$Z$};
\node [io, below=of z] (x) {$X$};
\node [deterministic, left=of x] (theta) {$\theta$};
\node [deterministic, above=of z] (l) {$L$};
\draw [->] (x) -- (z);
\draw [->] (theta) -- (z);
\draw [->] (z) -- (l);
\end{tikzpicture}
        \caption{Initial Model}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
 \node [deterministic] (z) {$Z$};
\node [io, below=of z] (x) {$X$};
\node [deterministic, left=of x] (theta) {$\theta$};
\node [deterministic, above=of z] (l) {$L$};
\node [neuron, right=of x] (eps) {$\epsilon$};
\draw [->] (x) -- (z);
\draw [->] (theta) -- (z);
\draw [->] (z) -- (l);
\draw [->] (eps) -- (z);
\end{tikzpicture}
        \caption{Reparametrised Model}
    \end{minipage}
\end{figure}

Another way to solve this issue is to use the reparametrisation trick also called pathwise estimator  from Kingma \& Welling \cite{kingma} and write $z$ as a deterministic function of the model parameter $\theta$ and a random variable $\epsilon$: $z = f(\theta(x), \epsilon)$. We can then write:
$$p(z) = p(\epsilon) \Big|\frac{d\epsilon}{dz} \Big| \Rightarrow |p(\epsilon) d\epsilon| = |p(z|x) dz|$$
The gradient can then be propagated back to the input and the parameters through the deterministic node $Z$:
\begin{align}
\nabla_\theta \int p_\theta(z|x) g(z) dz &= \nabla_\theta \int p(\epsilon) g\big(f(\theta(x), \epsilon)\big) d\epsilon\\
&= \int p(\epsilon) \nabla_\theta g\big(f(\theta(x), \epsilon)\big) d\epsilon\\
&= \mathbb{E}_{p(\epsilon)}\Big[\nabla_\theta g\big(f(\theta(x), \epsilon)\big)\Big]
\end{align}

We can therefore rewrite the estimator as:

 \begin{align}
\tilde{\mathcal{L}}   &= \frac{1}{N}\sum_n  \mathbb{E}_{\epsilon \sim p(\epsilon)}\Big[  \log q(y_n|f(x_n, \epsilon)) - \beta  \log \frac{p(f(x_n, \epsilon)|x_n)}{r(f(x_n, \epsilon))}\Big]  \label{eq:lower2}
\end{align}

Note that if the the KL divergence can be calculated analytically as it is the case if both distribution are gaussian we can write more simply:

 $$ \tilde{\mathcal{L}}  = \frac{1}{N}  \sum_{n=1}^{N} \mathbb{E}_{\epsilon \sim p(\epsilon)}\Big[\log q(y_{n} |f(x_{n}, \epsilon))\Big] - \beta KL[p(Z|x_{n}), r(Z)]$$

To turn this into a minimisation problem, the obective function becomes:

 $$ J_{VIB} = \frac{1}{N}  \sum_{n=1}^{N} \mathbb{E}_{\epsilon \sim p(\epsilon)}\Big[-\log q(y_{n} |f(x_{n}, \epsilon))\Big] + \beta KL\big[p(Z|x_{n}), r(Z)\big]$$


The first part of of the lower bound correspond to the cost of constructing $Y$ under our model from $Z$, that is how well our samples from $p_\theta(z|x)$ explains the data $Y$. The second part is a penalty or regularisation term that correspond to the number of bits we need to construct $Z$.

\section{Application to Sequential Data}

We apply the variational information bottleneck method to find a good representation of a sequence rather than a single input. We will use recurrent neural networks to combine the inputs and pass the last hidden layer of the network through an information bottleneck to get a robust representation of the sequence. As we have seen in the first chapter there are different supervised learning problems for sequential data, we will see how to apply the variational information bottleneck in each setting.

\subsection{Many to many}
\begin{figure}[H]
\centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
\foreach \jlabel [count=\j, evaluate={\jj=int(\j-1); \jd=int(2 * \j);}]  in {t-1,  t, t+1}{
      \node [deterministic] at (\jd, 1) (h-\j) {$h_{\jlabel}$};
       \node [neuron] at (\jd + 1, 1.5) (z-\j) {$Z_{\jlabel}$};
      \node [io, above=of h-\j] (x-\j) {$X_{\jlabel}$};
      \node [io, below=of h-\j] (y-\j) {$Y_{\jlabel}$};
      \draw [->] (x-\j) -- (h-\j);
      \draw [->] (x-\j) -- (z-\j);
      \draw [->] (h-\j) -- (y-\j);
      \ifnum\j>1
          \draw [->] (h-\jj.east) -- (h-\j.west);
      \fi
} 
\node [left=of h-1](ldots) {\ldots};
\node [right=of h-3](rdots) {\ldots};
\draw [->] (ldots) -- (h-1);
\draw [->] (h-3) -- (rdots);
\end{tikzpicture}
\caption{Graphical model of many to many variational information bottleneck}
\end{figure}

In the many to many problem we try to find at each time step a representation $Z_t$ that is maximally compressive of each input $X_t$ and maximally predictive about future outputs. The objective function can therefore be written as:

$$ J_{IB} = \sum_t I(Z_t, Y_{t}) - \beta I(X_{t}, Z_t)$$
 
 The problem with this setting is that the representation $Z_t$ does not contain information about the past, therefore they won't be good predictor of the outputs. One way to solve this would be to have $Z_t$ depend on all previous inputs. However in that case the $Z_t$ won't be independent and therefore we wouldn't be able to decompose the objective function as we did.
 
\subsection{Many to one}

\begin{figure}[H]
\centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
 \node [io] (x) {$X_{1:T}$};
\node [neuron] at (-1, -0.8) (z) {$Z$};
\node [io, below=of x] (y) {$Y$};
\draw [->] (x) -- (z);
\draw [->] (x) -- (y);
\end{tikzpicture}
\caption{Graphical model of many to one variational information bottleneck}
\end{figure}

The many to one problem is similar to the original supervised problem. Indeed we can consider the whole sequence as a random variable and apply the information bottleneck on this. When implementing this we will use the output of a recurrent neural network and pass it through the information bottleneck to get a representation of the sequence that is maximally compressive on the input sequence while being maximally predictive on the output label.

\begin{figure}[H]
\centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
\foreach \jlabel [count=\j, evaluate={\jj=int(\j-1); \jd=int(2 * \j);}]  in {1,  2}{
      \node [deterministic] at (\jd, 1) (h-\j) {$h_{\jlabel}$};
      \node [io, above=of h-\j] (x-\j) {$X_{\jlabel}$};
      \draw [->] (x-\j) -- (h-\j);
      \ifnum\j>1
          \draw [->] (h-\jj.east) -- (h-\j.west);
      \fi
} 
\node [right=of h-2] (rdots){\ldots};
\draw [->] (h-2) -- (rdots);
\node [io, above=of h-3] (x-3) {$X_T$};
\node [deterministic, right=of rdots] (h-L) {$h_T$};
\draw [->] (rdots) -- (h-3);
\draw [->] (x-3) -- (h-3);
\node [neuron] at (7, 3.5) (z) {$Z$};
\draw [->] (x-1.north) -- (z);
\draw [->] (x-2.north) -- (z);
\draw [->] (x-3.north) -- (z);
\node [io, right=of h-L] (y) {$Y$};
\draw [->] (h-L) -- (y);
\end{tikzpicture}
\caption{Graphical model of many to one variational information bottleneck }
\end{figure}

When drawing the graphical model we display the hidden states of the generative model rather than the hidden state of the encoder although in practice we will use a recurrent neural network that also contains hidden states to get a representation of the input sequence. Similarly the variational approximation $q_\phi(y|z)$ is not shown in this graphical model as it is not part of the generative process.

The information bottleneck objective is then:
$$ J_{IB} = I(Z, Y) - \beta I(X_{1:T}, Z)$$
Whose lower bound objective under the variational approximation becomes:
 $$ J_{VIB} = \frac{1}{N}  \sum_{n=1}^{N} \mathbb{E}_{\epsilon \sim p(\epsilon)}[- \log q(y^{n} |f(x_{1:T}^n, \epsilon))] + \beta KL[p(Z|x_{1:T}^n), r(Z)]$$

\subsection{Sequence to Sequence}
Sequence to sequence problems have first been used in the context of recurrent neural network by Sutskever \cite{s2s}. The deterministic model compresses the whole input sequence into a hidden state from which the output sequence is generated. This makes it a good candidate for the information bottleneck framework as pulling a relevant representation out of the input is crucial to be able to generate an accurate output sequence. Recent models have included attention \cite{attention} models or other external memory mechanism to go around this bottleneck, we will discuss this in the conclusion.

\begin{figure}[H]
\centering
\begin{tikzpicture}[x=2cm, y=1.5cm]
 \node [io] (x) {$X_{1:T}$};
\node [neuron] at (-1, -0.8) (z) {$Z$};
\node [io, below=of x] (y) {$Y_{1:S}$};
\draw [->] (x) -- (z);
\draw [->] (x) -- (y);
\end{tikzpicture}
\caption{Sequence to sequence variational information bottleneck graphical model}
\end{figure}

The graphical model of this problem is also similar to the original information bottleneck graphical model if we consider the input and output sequences as random variables. Representing the hidden states of the graphical model we get: 


\begin{figure}[H]
\centering
\begin{tikzpicture}[x=0.95cm, y=0.6cm]
\foreach \jlabel [count=\j, evaluate={\jj=int(\j-1); \jd=int(2 * \j);}]  in {1,  2}{
      \node [deterministic] at (\jd, 1) (h-\j) {$h_{\jlabel}$};
      \node [io, above=of h-\j] (x-\j) {$X_{\jlabel}$};
      \draw [->] (x-\j) -- (h-\j);
      \ifnum\j>1
          \draw [->] (h-\jj.east) -- (h-\j.west);
      \fi
} 
\node [right=0.5cm of h-2] (rdots){\ldots};
\draw [->] (h-2) -- (rdots);
\node [deterministic, right=0.5cm of rdots] (h-L) {$h_T$};
\node [io, above=of h-L] (x-L) {$X_T$};
\draw [->] (rdots) -- (h-L);
\draw [->] (x-L) -- (h-L);

\foreach \jlabel [count=\j, evaluate={\jj=int(\j-1); \jd=int(2 * \j);}]  in {1, 2}{
      \node [deterministic] at (8 + \jd, 1) (hpred-\j) {$i_{\jlabel}$};
      \node [io, below=of hpred-\j] (ypred-\j) {$Y_{\jlabel}$};
      \draw [->] (hpred-\j) -- (ypred-\j);
      \ifnum\j>1
          \draw [->] (hpred-\jj.east) -- (hpred-\j.west);
      \fi
} 
\draw [->] (h-L) -- (hpred-1);
\node [right=0.5cm of hpred-2] (rdots){\ldots};
\draw [->] (hpred-2) -- (rdots);
\node [deterministic, right=0.5cm of rdots] (hpred-M) {$i_S$};
\node [io, below=of hpred-M] (ypred-M) {$Y_S$};
\draw [->] (rdots) -- (hpred-M);
\draw [->] (hpred-M) -- (ypred-M);
\node [neuron] at (10, 7) (z) {$Z$};
\draw [->] (x-1.north) -- (z);
\draw [->] (x-2.north) -- (z);
\draw [->] (x-L.north) -- (z);
\end{tikzpicture}
\caption{Sequence to sequence variational information bottleneck graphical model}
\end{figure}

The information bottleneck objective is then:
$$ J_{IB} = I(Z, Y_{1:S}) - \beta I(X_{1:T}, Z)$$
Whose lower bound objective under the variational approximation becomes:
 $$ J_{VIB} = \frac{1}{N}  \sum_{n=1}^{N} \mathbb{E}_{\epsilon \sim p(\epsilon)}[- \log q(y_{1:S}^n |f(x_{1:T}^n, \epsilon))] + \beta KL[p(Z|x_{1:T}^n), r(Z)]$$

\section{Model Implementation}

\subsection{Stochastic encoder $p_\theta(z|x_{1:T})$}
Like in \cite{vib} we choose our encoder to be a gaussian whose mean and variance will be given by the output of a neural network:
$$ p(z|x) \sim \mathcal{N}(z | f^\mu(x_{1:T}), f^\Sigma(x_{1:T}))$$ where $f^\mu$ and $f^\Sigma$ are given by a recurrent neural network.

Since the encoder is a gaussian we can use  $z = f(x, \epsilon) =  \mu(x) + \Sigma(x)^{1/2} \epsilon$ to apply the reparametrisation trick where $\epsilon \sim \mathcal{N}(0, I)$. 

\subsection{Monte Carlo Expectation}

In the lower bound we have to take an expectation with respect $p(\epsilon)$. To do so we take S Monte Carlo samples: 
 $$ \mathbb{E}_{\epsilon \sim p(\epsilon)}[- \log q(y_{n} |f(x_{n}, \epsilon))] \approx \frac{1}{S}\sum_s - \log q(y_{n} |f(x_{n}, \epsilon_s)) $$
 
 In practice, we use S = 1 or S = 12. Taking more samples would give a better estimate however since we are using stochastic gradient descent, we are already averaging on the batches and have a doubly stochastic gradient estimator.
 
\subsection{Decoder $q_\phi(y|z)$}

\paragraph{Discrete}
In the discrete case we use a simple logistic regression for the variational approximation of $p(y|z)$:

$$ q_\phi(y_n|z) \sim \mathcal{C}(y_n| \mathcal{S}(Wz + b)))$$ where $\mathcal{C}$ is a categorical distribution, $\mathcal{S}$ is the softmax function and $z = f(x_n, \epsilon_s)$

The vector $ p = \mathcal{S}(Wz + b) $ can be seen as the prediction $\hat{y}$ of our model. Taking the cross-entropy between those predictions and the true label is equivalent to minimising the reconstruction cost. We note that we do not use the generative model $p_\theta(y|x)$ to make predictions.

Taking the log we get:
\begin{align}
- \log q_\phi(y_n|z) &= - \log \prod_{i=1}^{K} p_i^{[y_n=i]}\\
&= - \sum_{i=1}^{K} \delta(y_n, i) \log p_i\\
&= - \sum_{i=1}^{K} y_{n, i} \log p_i
\end{align}

Where $K$ is the number of categories and $y_{n, i}$ the one-hot vector notation of $y_n$. This is the cross entropy loss.

\paragraph{Continous}
In this case where $y$ is continuous, once possibility is to normalise and round the output such as to turn it into a classification problem. We can then use the cross entropy loss. This has been shown to work well in practice \cite{draw} for input that can easily be binarised. However for regression problem we need to choose another decoder, for example:

$$ q_\phi(y_n|z) \sim \mathcal{N}(y_n| f^\mu(z), f^\Sigma(z)))$$ 

In this case the prediction can be interpreted as $\hat{y} = f^\mu(z)$

\subsection{Marginal $r_\psi(z)$}

As an approximation to the marginal we can use as a simple non-parametric distribution:
$$r_\psi(z) \sim \mathcal{N}(0, I)$$

However to improve the expressiveness of the marginal distribution on the latent, we can allow its parameters to be learned. 

$$r_\psi(z) \sim \mathcal{N}(\mu_r, \Sigma_r)$$

In the objective function we calculate the KL divergence from $p_\theta(z|x) \sim \mathcal{N}(\mu_p, \Sigma_p)$ to $r_\psi(z) \sim \mathcal{N}(\mu_r, \Sigma_r)$. Since both are assumed to be gaussians with diagonal variance we can use:

\begin{align}
KL[\mathcal{N}_p| \mathcal{N}_r] &= \frac{1}{2}(Tr(\Sigma_r^{-1}\Sigma_p) + (\mu_r - \mu_p)^T\Sigma_r^{-1}(\mu_p - \mu_r) - k + \log\frac{|\Sigma_r|}{|\Sigma_p|})\\
&= \frac{1}{2}(\sum_i \frac{\Sigma_{p, ii}}{\Sigma_{r, ii}} + \sum_i \frac{(\mu_{r_i} - \mu_{p, i})^2}{\Sigma_{r, ii}} - k + \log\frac{\prod \Sigma_{r, ii}}{\prod \Sigma_{p, ii}})\\
&= \frac{1}{2}(\sum_i[ \log \Sigma_{r, ii} - \log \Sigma_{p, ii} - 1 + \frac{\Sigma_{p, ii}}{\Sigma_{r, ii}} + \frac{(\mu_{r_i} - \mu_{p, i})^2}{\Sigma_{r, ii}}]
\end{align}

In the case where $r_\psi(z)$ is a standard normal prior we get:
\begin{align}
KL[\mathcal{N}_p | \mathcal{N}_0] &= \sum_i[ -\log \Sigma_{p, ii} - 1 + \Sigma_{p, ii}+ \mu_{p, i}^2]
\label{eq:kl}.
\end{align}

\subsection{Computational Graph}
\begin{center}
\includegraphics[scale=0.4]{model}
\captionof{figure}{Computational Graph of the Sequence to Sequence Variational Information Bottleneck. The loss layers are indicated in blue and the stochastic layers in red.}
\end{center}

\paragraph{Input Sequence}
The input sequence is passed through a recurrent neural network consisting of GRU cells. The output of the last layer is then passed to the stochastic encoder.

\paragraph{Stochastic Encoder}
The encoder takes as input the output of the last layer of the recurrent neural network. We apply a linear transformation to get a vector of twice the size of the bottleneck. The output is then split in two, the first part is used to get the mean of the gaussian distribution and the second part to get the diagonal terms of the covariance matrix after passing through a softplus function. The covariance is chosen to be diagonal for computational efficiency, as a consequence the features represented by $Z$ cannot be correlated. The encoding $z$ is then found by applying the reparametrisation trick transformation to a sample $\epsilon$ from the standard normal.

\paragraph{Decoder}
A linear transformation is applied to the encoding $z$  to get an vector of the size of the hidden layer of the decoding RNN. The decoding RNN uses this vector as an initial state and provides an output at each step. Those output are linearly transformed to get a vector of the size of the number of labels. 

\paragraph{Loss}
The KL divergence is calculated from the encoder mean and variance and compared to the standard normal using equation \ref{eq:kl}. The reconstruction cost is calculated using a cross-entropy loss between the true label and the categorical probabilities given by the decoder.

\subsection{Architecture Details }
\paragraph{Exponential Moving Average}
To get more stable test results we keep track of the exponential moving average of the parameter values during training and use those at test time. The average values are defined as

$$w_{avg}^{k+1} = \tau w_{avg}^k  + (1 - \tau) w^k$$

Where $\tau$ is the decay constant that we took as 0.999.

\paragraph{Software Implementation}
We use TensorFlow to implement our models and run them on a single GPU. We use stochastic gradient descent to optimise our loss function with a batch size of 100 and a learning rate of $10^{-4}$. The source code is available on \href{https://github.com/tristanguigue/vib_s2s}{github}\footnote{https://github.com/tristanguigue/vib\_s2s}.

%------------------------------------------------------------------------------------------------
\chapter{Experiments and Results}
\section{One to one}
\subsection{MNIST image to label}

In this experiment we try to predict the label of a hand-written digit MNIST \cite{mnist} image as in the Deep Variational Information Bottleneck paper \cite{vib} using the same parameters. The objective is to reproduce the results and verify that the information bottleneck method is effective in this simple setup.

\begin{minipage}{0.5\textwidth}
        \centering
\begin{tabular}{ c | c  }
 Hidden units & 1024 \\
 Bottleneck size & 256 \\
\end{tabular}
\captionof{figure}{Parameters}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
\begin{tabular}{ c | c c }
 $\beta$ & accuracy & loss \\
 \hline
0 & 98.48 & 0.058 \\
$10^{-7}$ & 98.58 & 0.057\\
$10^{-6}$ & 98.55 & 0.060\\
$10^{-5}$ & 98.67 & 0.058\\
$10^{-4}$ & 98.71 & 0.053 \\
$10^{-3}$ & 98.68 & 0.059 \\
$10^{-2}$ & 98.42 & 0.095 \\
\end{tabular}
\captionof{figure}{Results}
    \end{minipage}

\begin{center}
\begin{tikzpicture}[scale = 0.85]
\begin{semilogxaxis}[
    xlabel={$\beta$},
    ylabel={Accuracy},
    xmin=0.00000001, xmax=0.01,
    ymin=98, ymax=100,
    xtick={0.00000001,0.0000001,0.000001,0.00001,0.0001,0.001,0.01},
    xticklabels={0, $10^{-7}$, $10^{-6}$, $10^{-5}$, $10^{-4}$, $10^{-3}$, $10^{-2}$},
    ytick={98, 99, 100},
    ymajorgrids=true,
    grid style=dashed,
]
\addplot[color=blue, mark=square,]
    coordinates {
   (0.00000001,98.48) (0.0000001,98.58)(0.000001,98.55)(0.00001,98.67)(0.0001,98.71)(0.001,98.68)(0.01,98.42)
    }; 
\end{semilogxaxis}
\end{tikzpicture}
\captionof{figure}{Accuracy vs Regularisation Parameter}
\end{center}

Although we don't obtain the same values as in \cite{vib}, our regularisation curve has the same shape: when increasing the regularsisation, the accuracy initially increases until it reaches an optimal level. It then decreases once the regularisation becomes too strong. In our case the best accuracy is achieved for $\beta = 10^{-4}$ with an error of 1.29.


\section{Many to one}
\subsection{Sequence of MNIST images to single label}
In this experiment we try to predict the label of a first image of a sequence of MNIST images. The network has to "remember" the first image it has seen and associate a label to it. It should learn to discard all the information coming from the rest of the sequence.

\begin{minipage}{0.5\textwidth}
        \centering
\begin{tabular}{ c | c  }
 GRU units & 128 \\
 Bottleneck size & 32 \\
 Sequence length & 5 \\
\end{tabular}
\captionof{figure}{Parameters}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
\begin{tabular}{ c | c c }
 $\beta$ & accuracy & loss \\
 \hline
0 & 93.70 & 0.278 \\
$10^{-5}$ & 95.10 & 0.279 \\
$10^{-4}$ & 94.75 & 0.278 \\
$10^{-3}$ & 95.60 & 0.274 \\
$10^{-2}$ & 95.75 & 0.271 \\
$10^{-1}$ & 93.50 & 0.42 \\
\hline
\hline
Dropout (0.95) & 95.50 & 0.253\\
Dropout (0.75) & 96.00 & 0.219\\
$p = 0.95$ and  $\beta = 10^{-2}$ &  96.40 & 0.19

\end{tabular}
\captionof{figure}{Results}
    \end{minipage}

\begin{center}
\begin{tikzpicture}[scale = 0.85]
\begin{semilogxaxis}[
    xlabel={$\beta$},
    ylabel={Accuracy},
    xmin=0.000001, xmax=0.1,
    ymin=93, ymax=100,
    xtick={0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1},
    xticklabels={0, $10^{-5}$, $10^{-4}$, $10^{-3}$, $10^{-2}$, $10^{-1}$},
    ytick={93, 94, 95, 96, 97, 98, 99, 100},
    ymajorgrids=true,
    grid style=dashed,
]
\addplot[color=blue, mark=square,]
    coordinates {
   (0.000001, 93.70)(0.00001, 95.10)(0.0001, 94.75)(0.001, 95.60)(0.01, 95.75)(0.1, 93.60)
    }; 
\end{semilogxaxis}
\end{tikzpicture}
\captionof{figure}{Accuracy vs Regularisation Parameter}
\end{center}

Again we see the same pattern with strong benefit of the information bottleneck regularisation with an optimal value of $\beta = 10^{-2}$ with an accuracy of 95.75. We see that we obtain similar results in terms of accuracy using a dropout regulariser although dropout gives smaller errors on the test set. Combining both regulariser gives the highest accuracy and lowest test loss values.

\section{Sequence to sequence}
\subsection{Sequence of MNIST images to sequence of labels}

\begin{center}
\includegraphics[scale=0.4]{img2label}
\captionof{figure}{Sequence to sequence MNIST experiment}
\end{center}

In this experiment we give a sequence of images and the objective is to generate the sequence of labels associated to those images in the right order. The encoder RNN and the decoder RNN can have different sizes.

\subsubsection{Random order}
We first consider the case where the images are taken at random with no particular order.

\begin{minipage}{0.5\textwidth}
        \centering
\begin{tabular}{ c | c  }
 Hidden units input & 512 \\
 Hidden units output & 256 \\
 Bottleneck size & 256 \\
 Sequence length & 5 \\
\end{tabular}
\captionof{figure}{Parameters}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
\begin{tabular}{ c | c c }
 $\beta$ & accuracy & loss \\
 \hline
0  & 92.31 & 0.381 \\
$10^{-5}$  & 94.03 & 0.399 \\
$10^{-4}$  & 94.59 & 0.380 \\
$10^{-3}$  & 94.52 & 0.379 \\
\hline
\hline
Dropout ($0.95$) & 94.74 & 0.383\\
Dropout ($0.75$) & 94.72 & 0.322
\end{tabular}
\captionof{figure}{Results}
    \end{minipage}

\begin{center}
\begin{tikzpicture}[scale = 0.85]
\begin{semilogxaxis}[
    xlabel={$\beta$},
    ylabel={Accuracy},
    xmin=0.000001, xmax=0.001,
    ymin=92, ymax=100,
    xtick={0.000001, 0.00001, 0.0001,0.001},
    xticklabels={0, $10^{-5}$, $10^{-4}$, $10^{-3}$},
    ytick={92, 93, 94, 95, 96, 97, 98, 99, 100},
    ymajorgrids=true,
    grid style=dashed,
]
\addplot[color=blue, mark=square,]
    coordinates {
   (0.000001, 92.31)(0.00001, 93.51)(0.0001, 94.59)(0.001, 94.52)
    }; 
\end{semilogxaxis}
\end{tikzpicture}
\captionof{figure}{Accuracy vs Regularisation Parameter}
\end{center}

As the results indicate, the information bottleneck method was successful in giving better accuracies in the regularised case with on optimium found for $\beta = 10^{-4}$. Using dropout regularisation gives slightly better results in terms of accuracy.

\begin{center}
\vspace{1.0cm}
\begin{minipage}{0.5\textwidth}
        \centering
\includegraphics[scale=0.28]{overfit_acc}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
\includegraphics[scale=0.28]{overfit_loss}
    \end{minipage}
\includegraphics[scale=0.28]{train_loss}
\captionof{figure}{Learning curves of test accuracy, test loss and train loss for $\beta = 0$ (light green) and $\beta = 10^{-4}$ (dark green) }
\label{fig:overfit}
\vspace{1.0cm}
\end{center}

Looking at the learning curves of this experiment (figure \ref{fig:overfit}), we see that using the information bottleneck regulariser indeed helps prevent overfitting:  in the case where $\beta$ is zero, the training loss goes down quickly to zero while the test loss starts increasing after 300 steps and the accuracy stays constant. With $\beta$ non zero, the training loss decreases more slowly and the test loss stays constant while the accuracy keeps increasing.

The fact that the accuracy in the regularised case keeps on rising even though the loss is nearly constant means that parameters are learned in a way that more images "flip" to the correct label although the overall loss is not increasing as some parameter update might worsen the probability to get a given label without changing the argmax output. Overall our final metric is the accuracy but the loss gives us insights on the learning process.

We notice that the training error is still very small in the regularised case which seem to indicate that our model is still too complex. Larger values of $\beta$ do increase the train error however at the expense of test accuracy.

\subsubsection{Increasing order}
In this experiment the images are ranked with respect to their labels so an example of sequence will be 2, 5, 6, 8, 8. This means the recurrent network can used previous digits to improve the guess on the current digit.

\begin{minipage}{0.5\textwidth}
        \centering
\begin{tabular}{ c | c  }
 Hidden units input & 512 \\
 Hidden units output & 256 \\
 Bottleneck size & 256 \\
 Sequence length & 5 \\
\end{tabular}
\captionof{figure}{Parameters}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
\begin{tabular}{ c | c c }
 $\beta$ & accuracy & loss \\
 \hline
0  & 94.53 & 0.217 \\
$10^{-5}$  & 95.72 & 0.220 \\
$10^{-4}$  & 96.34 & 0.212 \\
$10^{-3}$  & 96.06 & 0.219 \\
\hline
\hline
Dropout (0.95) & 96.60 & 0.19
\end{tabular}
\captionof{figure}{Results}
    \end{minipage}

\begin{center}
\begin{tikzpicture}[scale = 0.85]
\begin{semilogxaxis}[
    xlabel={$\beta$},
    ylabel={Accuracy},
    xmin=0.000001, xmax=0.001,
    ymin=94, ymax=100,
    xtick={0.000001, 0.00001, 0.0001,0.001,0.01},
    xticklabels={0, $10^{-5}$, $10^{-4}$, $10^{-3}$},
    ytick={94, 95, 96, 97, 98, 99, 100},
    ymajorgrids=true,
    grid style=dashed,
]
\addplot[color=blue, mark=square,]
    coordinates {
   (0.000001, 94.53)(0.00001, 95.72)(0.0001, 96.34)(0.001, 96.06)
    }; 
\end{semilogxaxis}
\end{tikzpicture}
\captionof{figure}{Accuracy vs Regularisation Parameter}
\end{center}

The overall accuracy is higher as the problem has become easier to solve however its behavior of the accuracy with respect to the regularisation parameter is very similar to previous results.

\subsubsection{CNN - RNN Architecture}
A Convolutional Neural Network (CNN) is a useful architecture to process images. They preserve translation invariance and if pooling layers are used, scale invariance. In this experiment, instead of providing as input to the RNN the raw images (784 pixels) we pass the images trough a CNN and provide the last layer of the CNN as inputs to the RNN.

Our convolutional neural network is composed of two convolutional layer each followed by a max-pooling layers to reduce the spatial resolution. Finally the output is turned into a single dimension vector and fed to the RNN. We won't describe in detail the functioning of a CNN, the main point is that we gather for each image relevant information on which digit it contains before feeding it to the RNN rather than feeding the raw image. We would like to see the effect it has on using the information bottleneck method.

The images do not have a particular order in this experiment.

\begin{minipage}{0.5\textwidth}
        \centering
\begin{tabular}{ c | c  }
 Hidden units input & 512 \\
 Hidden units output & 64 \\
 Bottleneck size & 64 \\
 Kernel size (channels) & 16\\
 Sequence length & 5 \\
\end{tabular}
\captionof{figure}{Parameters}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
\begin{tabular}{ c | c c }
 $\beta$ & accuracy & loss \\
 \hline
0  &  99.27  & 0.032 \\
$10^{-5}$  & 99.34 & 0.030  \\
$10^{-4}$  & 99.34 & 0.032  \\
$10^{-3}$  & 99.23 & 0.047  \\
\hline
\hline
Dropout (0.95) & 99.27 & 0.039
\end{tabular}
\captionof{figure}{Results}
    \end{minipage}

\begin{center}
\begin{tikzpicture}[scale = 0.85]
\begin{semilogxaxis}[
    xlabel={$\beta$},
    ylabel={Accuracy},
    xmin=0.000001, xmax=0.001,
    ymin=99, ymax=100,
    xtick={0.000001, 0.00001, 0.0001,0.001},
    xticklabels={0, $10^{-5}$, $10^{-4}$, $10^{-3}$, $10^{-2}$},
    ytick={99, 100},
    ymajorgrids=true,
    grid style=dashed,
]
\addplot[color=blue, mark=square,]
    coordinates {
   (0.000001, 99.27)(0.00001, 99.34)(0.0001, 99.34)(0.001, 99.23)
    }; 
\end{semilogxaxis}
\end{tikzpicture}
\captionof{figure}{Accuracy vs Regularisation Parameter}
\end{center}

We see that this model greatly outperforms previous ones, because the CNN captures spatial information and each layer learn a higher level representation of the image. The final layer that we feed the RNN contain the most relevant information about the label in the image. Passing it through the information bottleneck has less of an effect since the input has already been "compressed" by the CNN therefore reducing its entropy.

\subsection{Pixel Sequence Prediction}
\begin{center}
\includegraphics[scale=0.4]{next_pixel}
\captionof{figure}{Next pixels prediction experiment}
\end{center}

In this experiment we provide as input a sequence of pixel corresponding to part of the MNIST image. We try to predict the next 15 pixels. At both train and test time, we use the true value of the pixel as inputs to the decoder rather than using the pixels generated by the previous layer. This allows faster and more stable training.

\begin{minipage}{0.5\textwidth}
        \centering
\begin{tabular}{ c | c  }
 Hidden units input & 128 \\
 Hidden units output & 8 \\
 Bottleneck size & 8 \\
 Input sequence length & 60 \\
 Output sequence length & 15 \\
\end{tabular}
\captionof{figure}{Parameters}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
\begin{tabular}{ c | c c }
 $\beta$ & accuracy & loss \\
 \hline
0  &  97.49  & 0.060 \\
$10^{-5}$  & 97.51 & 0.061  \\
$10^{-4}$  & 97.52 & 0.062  \\
$10^{-3}$  & 97.47 & 0.065  \\
\hline
\hline
Dropout (0.95) & 97.32 & 0.065
\end{tabular}
\captionof{figure}{Results}
    \end{minipage}
    
    \begin{center}
\begin{tikzpicture}[scale = 0.85]
\begin{semilogxaxis}[
    xlabel={$\beta$},
    ylabel={Accuracy},
    xmin=0.000001, xmax=0.001,
    ymin=97, ymax=100,
    xtick={0.000001, 0.00001, 0.0001,0.001},
    xticklabels={0, $10^{-5}$, $10^{-4}$, $10^{-3}$},
    ytick={97, 98, 99, 100},
    ymajorgrids=true,
    grid style=dashed,
]
\addplot[color=blue, mark=square,]
    coordinates {
   (0.000001, 97.49)(0.00001, 97.51)(0.0001, 97.52)(0.001, 97.47)
    }; 
\end{semilogxaxis}
\end{tikzpicture}
\captionof{figure}{Accuracy vs Regularisation Parameter}
\end{center}

While we see a small improvement in terms of accuracy, the advantage of the information bottleneck regularisation is less obvious in this experiment. This might be due to the low dimensionality of the input. When experimenting with a larger bottleneck (32 units) we saw no benefit in using the information bottleneck method, indicating that restriction on the mutual information between $X$ and $Z$ seems less effective when the dimension of the bottleneck is large and the input low dimensional.

%------------------------------------------------------------------------------------------------
\chapter{Conclusion}
\section{Discussion of the results}
As we have seen on the different experiment performed, the information bottleneck method can significantly improve the accuracy of the algorithm in a sequence to sequence as well as in a sequence to label setting. It is especially effective when the inputs of the sequence are high dimensional as the choice of the representation becomes more important.

Looking at the change of the optimimum accuracy while changing the regularisation parameter shows that initially increasing the regularisation parameter helps getting a better acuracy on the test set as the regularisation layer gets rid of noise coming from the input. As the regularisation gets stronger we start losing useful information coming from the input and the accuracy decreases. 

We also notice that when dealing with categorical data, this type of regularisation did not necessarily induced lower losses on the test set but did helped keeping the test loss low and increased the accuracy.

In our model, the parameters of the stochastic encoding of the input sequence is given by the output of a recurrent neural network, the model is therefore very dependent on how the information flows through the network. We have seen that reducing the entropy of the input with a convolutional neural network reduced the efficiency of the information bottleneck regularisation.

Finally the results obtained with the information bottleneck regularisation were comparable to using RNN dropout regularisation. We also saw that as those regularisers act in different ways, it was possible to combine them to obtain a better performance.

\subsection{Attention Models}
State of the art sequence to sequence model for translation and other applications uses attention mechanisms to improve performance. The attention mechanism acts as an external memory to the decoder, at each of the decoder time step, the attention mechanism gives the possibility to go over the inputs that are the most relevant to predict the next output. As the information contained by the input sequence does not have to be contained in the first hidden state of the decoder network, the information bottleneck model does not apply. In fact the only information the "bottleneck" vector needs to contain is the information regarding where to concentrate the attention to predict the first output. Therefore in this dissertation we do not attempt to compare results using attention models.

\subsection{Interpretability}
Using the variational information bottleneck method helps providing some information in terms of what needs to be encoded such as to predict the output variable. This can be useful in term of interpretability: if it is often difficult to explain the mapping found between two variables especially in the context of neural network. In this case we have a vector of fixed size whose components could be interpreted as features in the same way as a dimensionality reduction technique but optimised for the prediction of our output variable.

\section{Limitations}
\subsection{Short running time}
In some of the experiments, we have had to choose an arbitrary number of epochs before we stopped the experiment to keep them under a given computation and time budget even though the performance might still have been increasing. This is not optimal as different degrees of regularisation lead to different learning rates. Therefore by stopping early an experiment we might flaw the results.

\subsection{Lack of confidence interval}
For each experiment and each set of the parameters, there should have been a given number of runs done such as to provide a mean and standard deviation of the optimal values. This would have allowed to give us confidence intervals of the values and compare their significance. Given the time and computation budget, we only ran a single run per experiment and set of parameter. Some reassurance comes from the trends obtained by varying the regularisation parameters. Those trends tend to be consistents which indicates some coherence in our results. 

\subsection{Tightness of the bound}
To optimise our objective function, we have made two approximations that each adds inaccuracies. To ensure that our optimum is close to the true optimum we would need to do an analysis of the tightness of the bound. We could use Markov Chain Monte Carlo sampling to get an equilibrium distribution and compare it to the variational approximation.

\section{Further Work}
\subsection{CNN to CNN}
Recent advances in sequence to sequence learning \cite{cnn_s2s} have shown that using a fully convolutional architecture can outperform RNN in sequence to sequence learning. Most recent work add attention mechanisms to the CNN-CNN architecture however it would be possible to build a simple encoder-decoder architecture with a fixed vector in between similar to the RNN-RNN architecture. It would be interesting to study the effect of the information bottleneck regularisation in this setting.

\subsection{Normalising Flows}
Richer variational approximations allows tighter variational bound and better overall performance. In this dissertation we use simple approximations however recent work by Rezende et. al. \cite{norm_flow} has shown the benefit of using arbitrarily complex approximations constructed by a sequence of invertible transformations. Using such approximations could provide better results with the informational bottleneck method.

\subsection{Translation}
One of the main application of sequence to sequence modelling is translation. We have attempted to use the information bottleneck method for sequence to sequence translation on the WMT'15 English-French dataset to be able to compare it with the original sequence to sequence model used by Sutskever et al. [7]. However the results obtained in terms of perplexity where far from the simpler model. Given the size of the dataset it is possible that longer running time would solve the issue, this would need to be verified in further work. 

\appendix
%------------------------------------------------------------------------------------------------
\chapter{Derivations}
\section{Derivations of the lower bounds}
\subsection{Lower bound of $I(X, Z)$}
\label{appendix:xz}
Given that the KL divergence is necessarily positive we have 

$$ KL\big[p(y|z)|q_\phi(y|z)\big] \geq 0$$ 
\begin{flalign*}
\Rightarrow \quad \int p(y|z) \log \frac{p(y|z)}{q_\phi(y|z)} dy & \geq 0 &\\
\Rightarrow \quad  \int p(y|z) \log p(y|z) dy &\geq \int p(y|z) \log q_\phi(y|z) dy&\\
\Rightarrow \quad \frac{1}{p(z)} \int p(y, z) \log p(y|z) dy &\geq  \frac{1}{p(z)} \int p(y, z) \log q_\phi(y|z) dy&\\
\Rightarrow \quad \int p(y, z) \log p(y|z) dy &\geq  \int p(y, z) \log q_\phi(y|z) dy&\\
\Rightarrow \quad \int p(y, z) \log p(y|z) dy\, dz &\geq  \int p(y, z) \log q_\phi(y|z) dy\, dz&\\
\Rightarrow \quad \int p(y, z) [\log p(y|z) - \log p(y)] dy\, dz & \geq  \int p(y, z) [\log q_\phi(y|z) - \log p(y)] dy\, dz &
\end{flalign*}
And
 \begin{align}
 \Rightarrow I(Z, Y) &\geq  \int p(y, z) \log q_\phi(y|z) dy\, dz + H(Y)\\
	  &=  \int p(x, y, z) \log q_\phi(y|z) dy\, dz\, dx + H(Y)\\
	  &=  \int p(x) p(y|x) p_\theta(z|x) \log q_\phi(y|z) dy\, dz\, dx + H(Y)
\end{align}
\subsection{Lower bound of $I(Z, Y)$}
\label{appendix:zy}
$$ KL[p(z)|r(z)] \geq 0$$
Therefore 
\begin{flalign*}
\int p(z) \log \frac{p(z)}{r(z)} dz &\geq 0 &\\
\Rightarrow \quad \int p(z) \log p(z) dz&\geq \int r(z) \log p(z) dz&\\
\Rightarrow \quad \int p(z, x) \log p(z) dx\, dz &\geq \int r(z, x) \log p(z) dx\, dz&\\
\Rightarrow \quad -\int p(z, x) \log p(z) dx\, dz &\leq - \int r(z, x) \log p(z) dx\, dz&\\
\Rightarrow \quad \int p(z, x) [\log p(z|x) -  \log p(z)]dx\, dz &\leq  \int p(z, x) [\log p(z|x) - \log p(z)]dx\, dz  &\\
\end{flalign*}
And
 \begin{align}
I(X, Z) & \leq  \int p(z, x) \log \frac{p(z|x)}{r(z)} dx\, dz\\
   	  & =  \int p(x, y, z) \log \frac{p(z|x)}{r(z)}dx\, dy\, dz \\
   	  & = \int p(x)p(y|x)p(z|x) \log \frac{p(z|x)}{r(z)}dx\, dy\, dz \\
\end{align}

\chapter{Other Experiments}
\section{MINST image to label further analysis}
\subsection{Behavior when $\beta = 0$}
When $\beta = 0$ we expect the network to become fully deterministic as there is no restriction on the variance of $p_\theta(z|x)$, the lower bound will be maximum when $z = \mu(x)$  that is having $\Sigma$ to be zero. We verify in this experiment that the values of the diagonal covariance matrix tend to zero as the network is trained.

\begin {table}[H]
\begin{center}
\begin{tabular}{ c | c c }
 $\beta$ & $\bar{\Sigma_{ii}}$ \\
 \hline
0 & 0.02\\
1 & 0.99 \\
\end{tabular}
\end{center}
\caption{Average covariance diagonal terms when $\beta$ varies}
\end {table}

After training the network for 200 epochs the average diagonal values of the covariance matrix are indeed close to 0 in the non-regularised case whereas with a very strong regularisation they will be very close to the variational parameters: $r(z) \sim \mathcal{N}(0, I)$

\subsection{Bottleneck size considerations}
In this experiment we change the size of the bottleneck while setting $\beta = 0$, that is we don't explicitly constrain the mutual information between $X$ and $Z$, however we would like to verify that simply reducing the bottleneck size does not have the same effect as setting a non-zero regularisation parameter.

\begin {table}[H]
\begin{center}
\begin{tabular}{ c | c c }
 K & accuracy & loss \\
 \hline
256 & 98.26 & 0.064 \\
128 & 98.14 & 0.065 \\
64 & 98.12& 0.067 \\
32 & 98.09 & 0.066 \\
\end{tabular}
\end{center}
\caption{Performance of the network for different bottleneck sizes}
\end {table}

We see that reducing the bottleneck size does not improve the test accuracy and loss. This is a confirmation that using the mutual information to control the amount of information we let flow from the input to the output is a better quantifier and regulariser.

\subsection{Parametric Marginal}
To improve on the approximation of the marginal $p(z)$ we study the effect of adding two parameter, a mean and a diagonal covariance matrix. We redefined our approximation as

$$r_\psi(x) = \mathcal{N}(\mu_r, \Sigma_r)$$ 

Where $\mu_r$ and $\Sigma_r$ are parameters that can be learned. 

\begin {table}[H]
\begin{center}
\begin{tabular}{ c  c| c c }
 Marginal & $\beta$ & Accuracy & Loss \\
 \hline
Standard &  0 & 98.48 & 0.058 \\
Parametric & 0 & 98.46 & 0.057 \\
Standard &  $10^{-4}$ & 98.71 & 0.053 \\
Parametric & $10^{-4}$ & 98.58 & 0.063 \\
\end{tabular}
\end{center}
\caption{Performance of the network for different bottleneck sizes}
\end {table}

We don't see an improvement of the performance when using the richer approximation. We note that we're still restricting the marginal to be uni-modal and having uncorrelated features.

\section{Sequence of frames}

\begin{center}
\includegraphics[scale=0.6]{video}
\captionof{figure}{Sequence of frames experiment}
\end{center}

In this experiment we generated videos with different shapes (triangle or squares) moving inside a box under the law of gravity. They can freely leave the box on each side but not at the top or bottom and are given an initial momentum. The target $Y$ is the number of frames in which the instances of given shape are present.

For this problem we use a normal distribution as the decoder as explained in the model description. As in the CNN - RNN experiment we use a CNN to process the image before it is fed to the RNN to help with the shape recognition.

\begin{minipage}{0.5\textwidth}
        \centering
\begin{tabular}{ c | c  }
 Train samples & 600 \\
 Test samples & 400 \\
 Hidden units & 512 \\
 Bottleneck size & 256 \\
 Sequence length & 60 \\
 Frame size & 36 x 36
\end{tabular}
\captionof{figure}{Parameters}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
\begin{tabular}{ c | c }
 $\beta$ &  loss \\
 \hline
0  & 0.038 \\
$10^{-5}$  & 0.038 \\
$10^{-4}$  & 0.036 \\
$10^{-3}$  & 0.040 \\
\end{tabular}
\captionof{figure}{Results}
\end{minipage}

We see marginal improvement using the information bottleneck problem, however running time were not sufficient to allow the loss to stabilise. 

\section{Next pixel prediction with generated pattern}
In this experiment we generated binary data from the following pattern:

\begin{equation}
 x_{n+1} \sim 
     \begin{cases}
	\mathcal{B}(0.1)\quad \text{if} \quad\sign(\sum_{i = n - 5}^n x_i) = 1\\
	\mathcal{B}(0.9)\quad \text{if} \quad\sign(\sum_{i = n - 5}^n x_i) = -1
    \end{cases}
 \end{equation}
 
 Where $\mathcal{B}$ is the Bernoulli distribution. We would like to compare this simple model with the pixel prediction for MNIST.
 
 \begin{minipage}{0.5\textwidth}
        \centering
\begin{tabular}{ c | c  }
 Hidden units input & 128 \\
 Hidden units output & 8 \\
 Bottleneck size & 8 \\
 Input sequence length & 60 \\
 Output sequence length & 15 \\
\end{tabular}
\captionof{figure}{Parameters}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
\begin{tabular}{ c | c c }
 $\beta$ & accuracy & loss \\
 \hline
0  &  89.54  & 0.345 \\
$10^{-4}$  & 89.79 & 0.35  \\
$10^{-3}$  & 89.90 & 0.350  \\
\end{tabular}
\captionof{figure}{Results}
    \end{minipage}
    
In this experiment although we did get better accuracies in the regularised case, the learning curves did not clearly indicate that the information bottleneck helped prevent overfitting, in particular the train loss was higher in the non-regularised case. More investigation would be required to understand this behaviour.

\begin{thebibliography}{9} 
\bibitem{barber}
David Barber and Felix Agakov
\textit{The IM Algorithm : A variational approach to Information Maximization}
NIPS 2004
\bibitem{tishby} 
N. Tishby, F.C. Pereira, and W. Biale.
\textit{The information bottleneck method}.
37th annual Allerton Conference on Communication, Control, and Computing, pp. 368-377, 1999.
\bibitem{vib} 
A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy
\textit{Deep variational information bottleneck}.
 arXiv:1612.00410, 2016.
 \bibitem{kingma} 
 Diederik P. Kingma and Max Welling.
 \textit{Auto-encoding variational Bayes.}
  ICLR, 2014
 \bibitem{gru} 
 Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio.
 \textit{Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.}
  NIPS, 2014
 \bibitem{graves}
 Graves, Alex.
 \textit{Generating sequences with recurrent neural networks.}
 CoRR, abs/1308.0850, 2013
 
\bibitem{s2s}
Ilya Sutskever, Oriol Vinyals and Quoc V. Le
\textit{Sequence to Sequence Learning with Neural Networks}
NIPS 2014
\bibitem{attention}
Dzmitry Bahdanau, KyungHyun Cho and Yoshua Bengio
\textit{Neural Machine Translation by Jointly Learning to Align and Translate}
ICLR 2015

\bibitem{vae}
Diederick P. Kingman and Max Welling
\textit{Auto-encoding variational Bayes}
ICLR, 2015

\bibitem{mnist}
Y. LeCun, L. Bottou, Y. Bengio and P. Haffner
\textit{Gradient-based learning applied to document recognition}
IEEE 86 (11), 1998

\bibitem{adam}
D. P. Kingma and J. Ba
\textit{Adam: A method for stochastic optimization.}
 CoRR, abs/1412.6980, 2014
 
\bibitem{vem}
M J. Beal and Z. Ghahramani
\textit{The Variational Bayesian EM Algorithm for Incomplete Data}
Oxford University Press, 2003

\bibitem{draw}
D. Gregor, I. Danihelka, A. Graves, D. Rezende and D. Wierstra
\textit{Draw: A recurrent neural network for image generation}
IICV, 2015

\bibitem{cnn_s2s}
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin
\textit{Convolutional Sequence to Sequence Learning}
arXiv:1705.03122, 2017

\bibitem{norm_flow}
Danilo Jimenez Rezende and Shakir Mohamed
\textit{Variational Inference with Normalizing Flows}
arXiv:1505.05770, 2016

\bibitem{dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov
\textit{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}
Journal of Machine Learning Research 15, 2014

\bibitem{ib_ml}
Noam Slonim and Yair Weiss
\textit{Maximum Likelihood and the Information Bottleneck}
NIPS, 2002

\bibitem{gal}
Yarin Gal and Zoubin Ghahramani
\textit{A Theoretically Grounded Application of Dropout in Recurrent Neural Networks}
NIPS, 2016

\bibitem{score}
Rajesh Ranganath, Sean Gerrish, and David M Blei.
\textit{Black box variational inference.} 
In AISTATS, pages 814?822, 2014.

\end{thebibliography}
\end{document}

